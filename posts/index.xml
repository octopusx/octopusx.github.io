<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Developer and Self Hoster Blog</title>
        <link>https://blog.octopusx.de/posts/</link>
        <description>Recent content in Posts on Developer and Self Hoster Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <copyright>&lt;a href=&#34;https://creativecommons.org/licenses/by-nc/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CC BY-NC 4.0&lt;/a&gt;</copyright>
        <lastBuildDate>Mon, 06 Nov 2023 00:24:20 +0200</lastBuildDate>
        <atom:link href="https://blog.octopusx.de/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Teapot - host your own helm chart registry</title>
            <link>https://blog.octopusx.de/posts/teapot/</link>
            <pubDate>Mon, 06 Nov 2023 00:24:20 +0200</pubDate>
            
            <guid>https://blog.octopusx.de/posts/teapot/</guid>
            <description>For a while I wanted to host my own helm chart registry and automate my chart build, release and deployment processes. As a homelaber, a one man band, the more things happen on their own without my manual involvement the better. Self hosting to me also means self reliance and local storage for most things needed in order to build and run my services, and getting my own registry is getting one step closer to that goal.</description>
            <content type="html"><![CDATA[<p>For a while I wanted to host my own helm chart registry and automate my chart build, release and deployment processes. As a homelaber, a one man band, the more things happen on their own without my manual involvement the better. Self hosting to me also means self reliance and local storage for most things needed in order to build and run my services, and getting my own registry is getting one step closer to that goal.</p>
<p>When I first started my homelab journey and decided to go the kubernetes route, the <a href="https://github.com/k8s-at-home">k8s-at-home project</a> was not only a source of quality helm charts for me to deploy but also an inspiration for how to write reusable charts and manage my own helm chart repositories. Sadly the project has been archived since then as the main maintainer had struggled to keep up with the updates without sufficient help from the community. At the time I either didn&rsquo;t have the time or the skills to support the project. Today, a few years later, I feel like I may finally have what it takes to bring back at least a small part of the project alive, starting from their library chart. In this blog post I will walk you through how I&rsquo;ve set up a public Gitea instance to be used as a helm chart registry and git repository for myself and anyone else who wishes to use the charts.</p>
<p>Before we go further, a quick disclaimer that, for all intents and purposes, <a href="https://truecharts.org/">https://truecharts.org/</a> has taken the place that k8s-at-home used to have. It is a fantastic resource for helm charts not just to use with TrueNas Scale but on any homelab k8s cluster. I highly recommend checking it out. That is not however what this blog is about, we&rsquo;re going to explore the fun but also the complex side of self hosting. So let&rsquo;s crack on!</p>
<h2 id="primary-objective">Primary Objective</h2>
<p>I am still running a lot of helm charts from the k8s-at-home public repository (<a href="https://github.com/k8s-at-home/charts">https://github.com/k8s-at-home/charts</a>) and all of them are built based on their library chart (<a href="https://github.com/k8s-at-home/library-charts)">https://github.com/k8s-at-home/library-charts)</a>. Now, so far this hasn&rsquo;t been a problem as K8S releases haven&rsquo;t deprecated many APIs lately, but it is only a matter of time before a change will break compatibility with the library chart. As the project is no longer in development, we will host the library chart ourselves and build a simple pipeline to publish it to our own registry. This way we will be able to update the base library chart when needed and port the k8s-at-home charts on our new updated version.</p>
<p>For this we will:</p>
<ul>
<li>Deploy a dedicated instance of Gitea, and call it Teapot!</li>
<li>Deploy Gitea runners and connect them to the Gitea instance.</li>
<li>Clone and sanitize the library chart, then host it in our new Gitea instance.</li>
<li>Write a Gitea action to build and release the library chart.</li>
<li>Build and push the library chart to Gitea&rsquo;s built in registry.</li>
</ul>
<h2 id="gitea-deployment">Gitea Deployment</h2>
<p>We will be deploying Gitea on our usual K8S cluster, as always. We will use the <a href="https://gitea.com/gitea/helm-chart/">official Gitea helm chart</a> as a base. Their chart is very robust and highly customizable. All we need to do is import it into our deployment chart and run <code>helm install</code> on it.</p>
<p>Chart.yaml:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v2</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">name</span>: <span style="color:#ae81ff">teapot</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">description</span>: <span style="color:#ae81ff">A public instance of Gitea, notably a Teapot.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">type</span>: <span style="color:#ae81ff">application</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">version</span>: <span style="color:#ae81ff">0.1.0</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">appVersion</span>: <span style="color:#e6db74">&#34;1.20.5&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">maintainers</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Tomasz Fratczak</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">email</span>: <span style="color:#ae81ff">sre@octopusx.de</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">dependencies</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">gitea</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">version</span>: <span style="color:#ae81ff">9.5.1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">https://dl.gitea.io/charts/</span>
</span></span></code></pre></div><p>Here is my values file. I disabled all of the unnecessary bells and whistles, configured ingress as well as the SSH load balancer service (for SSH traffic ingress), storage, disabled built in databases and pointed it at my existing postgres instance.</p>
<p>values.yaml:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">gitea</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">image</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">gitea/gitea</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">tag</span>: <span style="color:#ae81ff">1.20.5</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ingress</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#ae81ff">traefik-210-external</span> <span style="color:#75715e">## Class of dedicated Traefik ingress controller for external connectivity</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">traefik.ingress.kubernetes.io/router.entrypoints</span>: <span style="color:#ae81ff">websecure,web</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">traefik.ingress.kubernetes.io/router.middlewares</span>: <span style="color:#ae81ff">default-redirect-https@kubernetescrd</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">teapot.octopusx.de</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">tls</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">secretName</span>: <span style="color:#ae81ff">&lt;tls-secret-name&gt;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#ae81ff">teapot.octopusx.de</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">service</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">ssh</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">type</span>: <span style="color:#ae81ff">LoadBalancer</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">persistence</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">size</span>: <span style="color:#ae81ff">30Gi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">redis-cluster</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">postgresql-ha</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">gitea</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">config</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">actions</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ENABLED</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">server</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">START_SSH_SERVER</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ROOT_URL</span>: <span style="color:#ae81ff">https://teapot.octopusx.de</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">service</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">DISABLE_REGISTRATION</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">cache</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ENABLED</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">session</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">PROVIDER</span>: <span style="color:#ae81ff">db</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">database</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">DB_TYPE</span>: <span style="color:#ae81ff">postgres</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">HOST</span>: <span style="color:#ae81ff">&lt;postgres-url&gt;:&lt;postgres-port&gt;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">NAME</span>: <span style="color:#ae81ff">teapot</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">USER</span>: <span style="color:#ae81ff">teapot</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">PASSWD</span>: <span style="color:#e6db74">&#34;&lt;PASSWORD&gt;&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">SCHEMA</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span></code></pre></div><p>For the database connection to be established I had pre-created a teapot database and user in my postges instance, like so:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">USER</span> teapot
</span></span><span style="display:flex;"><span>      LOGIN
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">ENCRYPTED</span> PASSWORD <span style="color:#e6db74">&#39;&lt;password&gt;&#39;</span>;
</span></span><span style="display:flex;"><span>	  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">DATABASE</span> teapot
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">ENCODING</span> <span style="color:#e6db74">&#39;UTF8&#39;</span>
</span></span><span style="display:flex;"><span>      LC_COLLATE<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;C&#39;</span>
</span></span><span style="display:flex;"><span>      LC_CTYPE<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;C&#39;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">template</span><span style="color:#f92672">=</span>template0
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">OWNER</span> teapot;
</span></span></code></pre></div><p>Armed with the database credentials, we can now deploy the chart:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>helm install teapot . --set gitea.gitea.postgresql.password<span style="color:#f92672">=</span>&lt;PASSWORD&gt; --set gitea.gitea.admin.password<span style="color:#f92672">=</span>&lt;PASSWORD&gt;
</span></span></code></pre></div><p>With the following result:
<img src="teapot_lens.png" alt="teapot lens"></p>
<p>We can now log into our new instance, in my case under <a href="https://teapot.octopusx.de">https://teapot.octopusx.de</a>, as <code>gitea_admin</code> with the password we just set and start messing around.</p>
<h2 id="gitea-runner">Gitea Runner</h2>
<p>Now that we have our Gitea instance up and running, we need to create a runner to be able to deploy the library chart.</p>
<p>The Gitea act runner is easiest deployed as a container. I won&rsquo;t get into too much detail on how you can confiture it as Gitea&rsquo;s documentation is very detailed already: <a href="https://docs.gitea.com/next/usage/actions/act-runner">https://docs.gitea.com/next/usage/actions/act-runner</a>. What I will point to in addition though is the examples folder in the gitea act runner repo, to be found here: <a href="https://gitea.com/gitea/act_runner/src/branch/main/examples">https://gitea.com/gitea/act_runner/src/branch/main/examples</a>. Having read this I have realised that it basically needs to orchestrate a docker daemon, docker-in-docker style. This is generally not a good idea inside of a Kubernetes cluster, for practical as well as security reasons. I have therefore opted to run this workload in its own LXC container on one of my proxmox servers instead. I installed docker inside of the of my LXC and ran the following command:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    -v $PWD/config.yaml:/config.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    -v $PWD/data:/data <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    -v /var/run/docker.sock:/var/run/docker.sock <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    -e CONFIG_FILE<span style="color:#f92672">=</span>/config.yaml <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    -e GITEA_INSTANCE_URL<span style="color:#f92672">=</span>https://teapot.octopusx.de:443 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    -e GITEA_RUNNER_REGISTRATION_TOKEN<span style="color:#f92672">=</span>&lt;get your own token lol&gt; <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    -e GITEA_RUNNER_NAME<span style="color:#f92672">=</span>act01 <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    -e GITEA_RUNNER_LABELS<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ubuntu-22.04:docker://node:18-bullseye&#34;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    --name act_runner <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    -d gitea/act_runner:nightly
</span></span></code></pre></div><p>If you&rsquo;re wandering how to get the registration token from your Gitea server, I recommend reading the quick start guide: <a href="https://docs.gitea.com/next/usage/actions/quickstart">https://docs.gitea.com/next/usage/actions/quickstart</a>.</p>
<p>And here we go, a newly registered runner, fresh out of the LXC and still warm:
<img src="act_runner.png" alt="Gitea Actions Runner"></p>
<h2 id="library-chart-repository">Library Chart Repository</h2>
<p>I don&rsquo;t think I need to tell you how to create a new repository in Gitea do I? To make scoping and access rights management easier in the future I created an organization first, called it octocloudlab. It is hard to find good names that aren&rsquo;t already copy righted, so I thought this one is just nonsense enough to get the job done. I then created a new repository called library-chart under this new organization. I did not want to just create a fork of k8s-at-home library-charts repository, as that was indeed a complex project that would be somewhat hard for one person to wrap their head around. The original repository was shared under the Apache 2.0 license, so I started by creating a README.md with the appropriate attributions to the original project contributors (<a href="https://teapot.octopusx.de/octocloudlab/library-chart/src/branch/main/ATTRIBUTION.md">which you can find here</a>). Then I tool just the stable common chart from here: <a href="https://github.com/k8s-at-home/library-charts/tree/main/charts/stable/common">https://github.com/k8s-at-home/library-charts/tree/main/charts/stable/common</a> to use as the blueprint for my release. BTW, this is the first time I am basing a project on someone else&rsquo;s work, if you think I could have done anything better when it comes to attribution or other license-related issues, please let me know on the socials.</p>
<p>Before I committed it to my new repository I made sure to update all of the metadata, removing any references to the original contributors and documentation from the code itself, in order to make sure there are no misunderstandings as to who the maintainer of this repository is and where one should go to seek help if necessary.</p>
<p>To update all of the auto-generated Helm documentation I then ran:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>helm-docs generate -t README_CHANGELOG.md.gotpl -t README_CONFIG.md.gotpl -t README.md.gotpl
</span></span></code></pre></div><p>from inside the <code>chart</code> folder, which produced the updated <code>README.md</code>.</p>
<p>The final result is a clean and simple, publicly available repository with just the necessary basics to allow me to maintain and update the common library chart for my own benefits.</p>
<p><img src="library_chart.png" alt="Library Chart"></p>
<p>Should you wish to explore it, feel free to visit <a href="https://teapot.octopusx.de/octocloudlab/library-chart/">https://teapot.octopusx.de/octocloudlab/library-chart/</a>.</p>
<h2 id="gitea-action">Gitea Action</h2>
<p>Gitea&rsquo;s actions system is drop-in compatible with Github actions. Even better, by default they use the Github actions repository and you can use use the same workflows and ready-made actions that you normally see on Github. For me this is a fantastic opportunity to try out and learn Github actions, which my company uses at work but I had no opportunity to play with. So, the first thing I decided to do is run a test demo workflow to make sure that my runners are operating correctly. I found this demo example on Gitea&rsquo;s blog here <a href="https://blog.gitea.com/feature-preview-gitea-actions/">https://blog.gitea.com/feature-preview-gitea-actions/</a>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">name</span>: <span style="color:#ae81ff">Gitea Actions Demo</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">run-name</span>: <span style="color:#ae81ff">${{ github.actor }} is testing out Gitea Actions üöÄ</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">on</span>: [<span style="color:#ae81ff">push]</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">jobs</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">Explore-Gitea-Actions</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">runs-on</span>: <span style="color:#ae81ff">ubuntu-latest</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">steps</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">run</span>: <span style="color:#ae81ff">echo &#34;üéâ The job was automatically triggered by a ${{ github.event_name }} event.&#34;</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">run</span>: <span style="color:#ae81ff">echo &#34;üêß This job is now running on a ${{ runner.os }} server hosted by Gitea!&#34;</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">run</span>: <span style="color:#ae81ff">echo &#34;üîé The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}.&#34;</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Check out repository code</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">actions/checkout@v3</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">run</span>: <span style="color:#ae81ff">echo &#34;üí° The ${{ github.repository }} repository has been cloned to the runner.&#34;</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">run</span>: <span style="color:#ae81ff">echo &#34;üñ•Ô∏è The workflow is now ready to test your code on the runner.&#34;</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">List files in the repository</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">run</span>: |<span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">          </span>          <span style="color:#ae81ff">ls ${{ github.workspace }}</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">run</span>: <span style="color:#ae81ff">echo &#34;üçè This job&#39;s status is ${{ job.status }}.&#34;</span>
</span></span></code></pre></div><p>I added this to <code>.gitea/workflows/demo.yaml</code> in my <code>library-chart</code> repo and&hellip; nothing. The action was waiting forever. This is the point where I have learned that actions and act runners need to have matching labels. In this case, the</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">runs-on</span>: <span style="color:#ae81ff">ubuntu-latest</span>
</span></span></code></pre></div><p>has to match the label <code>ubuntu-22.04:docker://node:18-bullseye</code> on my action runner in order for it to pick up the action. The interesting and unusual thing about this system is that this label (or tag) not only identifies the runner but also provides the specification of what the runner is capable of. In this case:</p>
<pre tabindex="0"><code>ubuntu-22.04 : docker       : // node:18-bullseye
Runner OS    : runner type  : docker image used
</code></pre><p>So, once we fixed that and labeled the job and the runner correctly, it triggered on the next push and executed no problem. Let&rsquo;s move onto the actual workflow that we want to build though. The strength of the actions model is that they are reusable and you can find a multitude of pre-configured workflows to use in your own pipelines straight on Github&rsquo;s &ldquo;marketplace&rdquo;: <a href="https://github.com/marketplace?type=actions">https://github.com/marketplace?type=actions</a>. What I tried to do was:</p>
<ul>
<li>check out the project</li>
<li>package up the helm chart</li>
<li>push the new package to the gitea registry</li>
</ul>
<p>To check out the repo we can use the same action as the demo action, i.e. <code>actions/checkout@v4</code>. I then tried using plain <code>run</code> commands that allow you to execute arbitrary bash code, but my action was slowly becoming pretty complex. This is because I would have to:</p>
<ul>
<li>install helm</li>
<li>install helm plugin cm-push</li>
<li>add my gitea registry as helm repository</li>
<li>package the chart</li>
<li>publish the package to the helm repository</li>
</ul>
<p>I started searching for a ready made Github action, thinking that someone must have needed this functionality at some point, and yeah, I found one pretty quickly. The <code>bsord/helm-push@v4.1.0</code> does all of the above in a single step with minimal configuration needed. It had some problems itself, as for some reason all of the documentation for it was listing a non-existent <code>4.2.0</code> tag, and without the <code>v</code>, so it took me a while to figure out why the heck it&rsquo;s failing. Finally though, I made it:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">name</span>: <span style="color:#ae81ff">Gitea Actions Demo</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">run-name</span>: <span style="color:#ae81ff">${{ gitea.actor }} is testing out Gitea Actions üöÄ</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">on</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">workflow_dispatch</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">push</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">branches</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">main</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#e6db74">&#39;chart/**&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">jobs</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">Explore-Gitea-Actions</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">runs-on</span>: <span style="color:#ae81ff">ubuntu-22.04:docker://node:18-bullseye</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">steps</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Check out repository code</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">actions/checkout@v4</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">Push Helm Chart to Gitea Registry</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">uses</span>: <span style="color:#ae81ff">bsord/helm-push@v4.1.0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">with</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">username</span>: <span style="color:#ae81ff">${{ secrets.USERNAME }}</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">password</span>: <span style="color:#ae81ff">${{ secrets.PUBLIC_PACKAGE_WRITE }}</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">registry-url</span>: <span style="color:#e6db74">&#39;https://teapot.octopusx.de/api/packages/octocloudlab/helm&#39;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">force</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">chart-folder</span>: <span style="color:#ae81ff">chart</span>
</span></span></code></pre></div><p>This is basically all I needed to get this basic workflow running. Now, this workflow will be triggered each time a change to the <code>chart</code> folder in the <code>main</code> branch is made. In order to be able to download and use this library chart you can now do the following:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>helm repo add octocloudlab https://teapot.octopusx.de/api/packages/octocloudlab/helm
</span></span><span style="display:flex;"><span>helm repo update
</span></span></code></pre></div><p>And to include it in your helm chart add the following to your dependencies section in the <code>Chart.yaml</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e">## Chart.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">dependencies</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">name</span>: <span style="color:#ae81ff">common</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">version</span>: <span style="color:#ae81ff">4.5.2</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">https://teapot.octopusx.de/api/packages/octocloudlab/helm</span>
</span></span></code></pre></div><h2 id="what-now">What now?</h2>
<p>Now that I have this library chart published and a way to modify it and publish patches reliably, I will slowly replace the current helm chart that I am using in my infrastructure with a version of them that is based on this library chart. Since at this point I haven&rsquo;t made any changes to the k8s-at-home library chart this should be a drop in replacement. Then, going forward, I will start adapting the library chart to my needs, adding capabilities that I am missing in my homelab and trying to patch it for newer versions of kubernetes API.</p>
<p>The first victim on the chopping block however will be the chart I pieced together to deploy <code>refact.ai</code>, about which you can read in my previous blog entry. As far as I know there is no official helm chart for that project, so it will be interesting to publish a nice clean helm chart for them and see if anyone, besides me, actually ends up using it.</p>
]]></content>
        </item>
        
        <item>
            <title>Kubes and GPUs</title>
            <link>https://blog.octopusx.de/posts/kubes_and_gpus/</link>
            <pubDate>Mon, 30 Oct 2023 13:00:00 +0200</pubDate>
            
            <guid>https://blog.octopusx.de/posts/kubes_and_gpus/</guid>
            <description>Graphics cards are once again available. The AI hype is on for sure, but it&amp;rsquo;s not as much of a craze as the the mining fad of yesteryear. Finally, graphics-based compute is semi-affordable again, especially if you look to the second hand market. Having dabbed my toes in the world of self hosting large language models I have come to realize that hosting those on CPUs only is not going to give me a good insight into what this tech is capable of.</description>
            <content type="html"><![CDATA[<p>Graphics cards are once again available. The AI hype is on for sure, but it&rsquo;s not as much of a craze as the the mining fad of yesteryear. Finally, graphics-based compute is semi-affordable again, especially if you look to the second hand market. Having dabbed my toes in the world of self hosting large language models I have come to realize that hosting those on CPUs only is not going to give me a good insight into what this tech is capable of. A lot of different models and APIs are either optimized or exclusively available on GPU platforms only. The one that tipped the scale for me is <a href="https://refact.ai">refact.ai</a>. I really wanted to give the self hosted version a spin, and so I had gone on to make my wallet slimmer.</p>
<h2 id="the-hardware">The Hardware</h2>
<p>What GPU you should buy to get the most out of it depends on the models that you want to run on it. There do exist smaller coding-focused LLMs that can be run on a GTX 1060 6GB or an 8GB GTX 1070, which can be sniped on the second hand market for under a 100 euros. AMD has historically sold cards with higher VRAM sizes which would make their second hand units the best bargain, sadly it seems that their <a href="https://rocmdocs.amd.com/en/latest/">ROCm software platform</a> for running AI compute tasks is still in its early days and not many, if any, consumer cards are supported at the moment.</p>
<p>Some of the available models and their estimated VRAM requirements:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>VRAM Requirement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Refact/1.6B</td>
<td>~4 GB</td>
</tr>
<tr>
<td>starcoder/15b/base</td>
<td>~9 GB</td>
</tr>
<tr>
<td>wizardcoder/15b</td>
<td>~9 GB</td>
</tr>
<tr>
<td>codellama/7b</td>
<td>~14.3 GB</td>
</tr>
</tbody>
</table>
<p>I was actually about to pick up a second hand GTX 1070 with 8GB of VRAM for 90 euros when I found a good deal on a refurbished RTX 3060 12GB model and ordered immediately for 270 euros delivered. In terms of performance per euro the 1070 was a much better choice, however the 3060 has 3 things going for it:</p>
<ul>
<li>lower power consumption (lol nope)</li>
<li>smaller physical footprint (I am trying to cram this into a mITX case)</li>
<li>extra 4GB of VRAM (more VRAM more better models)</li>
</ul>
<p>Fun fact, now that I already have the card and started writing this article, I realized that actually the 3060 has a higher peak power and similar idle numbers to the 1070&hellip; Oh well&hellip; According to techPowerUp anyway: 1070: 13W idle, 148 peak, 3060: 13W idle, 179 peak (<a href="https://www.techpowerup.com/review/evga-geforce-rtx-3060-xc/36.html)">https://www.techpowerup.com/review/evga-geforce-rtx-3060-xc/36.html)</a>.</p>
<p>A lot of models that I want to try out exceed the 8GB mark once loaded into RAM, notably <code>starcoder</code>. I was also curious to figure out what it would take to set up CUDA time sharing on a single GPU, where multiple smaller models are loaded into VRAM simultaneously. It is a shame that AMD is not an option, as they have a number of 16GB VRAM cards at a similar price to this one (RX6700XT for example), while for Nvidia we would have to jump up to an RTX 4060 16GB, which retails at around 500 euros.</p>
<h2 id="the-platform">The Platform</h2>
<p>You know what you&rsquo;re getting yourself into coming here right? It&rsquo;s all about Kubernetes xD One of the Proxmox nodes in my server rack has a spare X16 slot that will happily house our refurb 3060, and we can pass it through to one of the Kubernetes node VMs. To do this we need to follow a few steps:</p>
<h3 id="prepare-proxmox">Prepare Proxmox</h3>
<ol>
<li>Drain the node to be shut down</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl drain k8s12 --delete-emptydir-data --ignore-daemonsets
</span></span></code></pre></div><ol start="2">
<li>
<p>Edit the hardware definition of the node (add the new PCI device)
<img src="proxmox.png" alt="adding pcie device to vm in proxmox"></p>
</li>
<li>
<p>Shut down the node so that it may pick up the hardware change</p>
</li>
<li>
<p>(optional) Make a snapshot/backup of the VM, in case we screw something up later
<img src="proxmox_snapshot.png" alt="creating a snapshot of the VM we are about to modify"></p>
</li>
<li>
<p>Boot it up again</p>
</li>
</ol>
<p>Voila, the GPU should be now available inside the VM. We can check that by running:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~ ‚ùØ lspci                                                                             octopusx@k8s12
</span></span><span style="display:flex;"><span>00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC <span style="color:#f92672">[</span>Natoma<span style="color:#f92672">]</span> <span style="color:#f92672">(</span>rev 02<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA <span style="color:#f92672">[</span>Natoma/Triton II<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE <span style="color:#f92672">[</span>Natoma/Triton II<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>00:01.2 USB controller: Intel Corporation 82371SB PIIX3 USB <span style="color:#f92672">[</span>Natoma/Triton II<span style="color:#f92672">]</span> <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI <span style="color:#f92672">(</span>rev 03<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:02.0 VGA compatible controller: Device 1234:1111 <span style="color:#f92672">(</span>rev 02<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:03.0 Unclassified device <span style="color:#f92672">[</span>00ff<span style="color:#f92672">]</span>: Red Hat, Inc. Virtio memory balloon
</span></span><span style="display:flex;"><span>00:05.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI
</span></span><span style="display:flex;"><span>00:10.0 VGA compatible controller: NVIDIA Corporation GA106 <span style="color:#f92672">[</span>GeForce RTX <span style="color:#ae81ff">3060</span> Lite Hash Rate<span style="color:#f92672">]</span> <span style="color:#f92672">(</span>rev a1<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:11.0 Audio device: NVIDIA Corporation Device 228e <span style="color:#f92672">(</span>rev a1<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:12.0 Ethernet controller: Red Hat, Inc. Virtio network device
</span></span><span style="display:flex;"><span>00:1e.0 PCI bridge: Red Hat, Inc. QEMU PCI-PCI bridge
</span></span><span style="display:flex;"><span>00:1f.0 PCI bridge: Red Hat, Inc. QEMU PCI-PCI bridge
</span></span></code></pre></div><ol start="6">
<li>Uncordon the K8S node to re-enable scheduling on it</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl uncordon k8s12
</span></span></code></pre></div><h4 id="fun-fact">Fun fact</h4>
<p>As much as the above steps are all that I &ldquo;technically&rdquo; had to do should my proxmox server be correctly configured from the get go, I was getting weird behavior at the start. The graphics card was passed into the VM the first time it booted, but the driver (I installed next) would tell me there is no GPU available, even after rebooting the VM. I started messing around, thinking it is maybe some weird property of PCIe passthrough on Nvidia hardware, went on to blacklist the nvidia kernel module/drivers inside the Proxmox host to make sure it doesn&rsquo;t get initialized by the host, which in turn prevented the VM I would pass it through from booting altogether. In the end I jumped into the bios of the machine to make sure all of the PCIe-related settings are in order and&hellip; of course I left the bifurcation settings on the x16 slot in the x8/x8 mode from when I was using multiple NVME drives in that slot. Changing that back and reverting all of the trial and error driver shenanigans on the host got it to work, boot and be available inside the VM every time.</p>
<h3 id="install-nvidia-drivers">Install Nvidia Drivers</h3>
<p>Now off to install all of that dirty proprietary Nvidia stuff. We only just gave them our money, so we&rsquo;ve got nothing to loose by going all the way and also giving them our souls.</p>
<p>Nvidia has a pretty solid guide with versions for various popular server linux flavors:</p>
<ul>
<li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html</a></li>
</ul>
<p>Which amounts to installing the CUDA toolkit and drivers.</p>
<ol>
<li>Install the CUDA toolkit</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
</span></span><span style="display:flex;"><span>sudo dpkg -i cuda-keyring_1.1-1_all.deb
</span></span><span style="display:flex;"><span>sudo apt-get update
</span></span><span style="display:flex;"><span>sudo apt-get -y install cuda-toolkit-12-3
</span></span></code></pre></div><ol start="2">
<li>Install Nvidia drivers</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt-get install -y nvidia-kernel-open-545
</span></span><span style="display:flex;"><span>sudo apt-get install -y cuda-drivers-545
</span></span></code></pre></div><ol start="3">
<li>Install the Nvidia container runtime</li>
</ol>
<p>The last step that we need to do to prepare our Ubuntu VM to run GPU accelerated tasks inside OCI containers is to install the Nvidia container runtime. This is a container runtime that is compatible with the OCI specification and is able to run GPU accelerated containers. It is also compatible with the Kubernetes CRI interface, which is what K3S uses to run containers. Once installed, K3S will automatically detect it once its service is restarted.
You can find out the setup details (which are very simple) on the following documentation page: <a href="https://docs.k3s.io/advanced#configuring-containerd">https://docs.k3s.io/advanced#configuring-containerd</a>.</p>
<p>Since we are running K3S on Ubuntu we are using systemd-containerd and installing everything from an apt repo. Therefore we need to follow Nvidia&rsquo;s instructions for enabling their container runtime for containerd, as found here: <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-containerd">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-containerd</a>. In my case that would be:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#f92672">&amp;&amp;</span> curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    sed <span style="color:#e6db74">&#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39;</span> | <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    sudo apt update
</span></span><span style="display:flex;"><span>    sudo apt install -y nvidia-container-toolkit
</span></span></code></pre></div><p>Once this is done you can run an Nvidia CUDA container to see that the runtime is functioning:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~ ‚ùØ sudo ctr image pull docker.io/nvidia/cuda:12.2.2-base-ubuntu22.04
</span></span><span style="display:flex;"><span>docker.io/nvidia/cuda:12.2.2-base-ubuntu22.04:                                    resolved       |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>index-sha256:0b165c469e8b0a620ce6b22373ead52502ef06d3088ba35a6edb78582b5274f6:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>manifest-sha256:1f0416abdf40fca3a4ce4e42093584664b4ac0dddd012571453c94e5c7a35937: <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>config-sha256:f5861cf44882f74454e1a5915647321b5b344af925503515bcd8bb0728d84551:   <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:aece8493d3972efa43bfd4ee3cdba659c0f787f8f59c82fb3e48c87cbb22a12e:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:4b46bd5b7766c7415446d5b866f709b45dbdb60b04fec7677343d8232ca2e427:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:f038740a8e0591897bdd6280d24e1f89690cf80c28e77c14072f84eca205c7e6:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:b2dca9da14b18f8a959e7deed652de532f88f979d8d1279511b987e294699f3c:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:998220aec9f5708b331daeb126f9d66fe39bbcc333224ca9ba6efabc9b563b7d:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>elapsed: 7.0 s                                                                    total:  83.6 M <span style="color:#f92672">(</span>11.9 MiB/s<span style="color:#f92672">)</span>                                      
</span></span><span style="display:flex;"><span>unpacking linux/amd64 sha256:0b165c469e8b0a620ce6b22373ead52502ef06d3088ba35a6edb78582b5274f6...
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">done</span>: 1.00494491s
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>~ ‚ùØ sudo ctr run --rm --gpus <span style="color:#ae81ff">0</span> -t docker.io/nvidia/cuda:12.2.2-base-ubuntu22.04 12.2.2-base-ubuntu22.04 nvidia-smi
</span></span><span style="display:flex;"><span>Thu Oct <span style="color:#ae81ff">26</span> 07:30:54 <span style="color:#ae81ff">2023</span>       
</span></span><span style="display:flex;"><span>+---------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |
</span></span><span style="display:flex;"><span>|-----------------------------------------+----------------------+----------------------+
</span></span><span style="display:flex;"><span>| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
</span></span><span style="display:flex;"><span>| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
</span></span><span style="display:flex;"><span>|                                         |                      |               MIG M. |
</span></span><span style="display:flex;"><span>|<span style="color:#f92672">=========================================</span>+<span style="color:#f92672">======================</span>+<span style="color:#f92672">======================</span>|
</span></span><span style="display:flex;"><span>|   <span style="color:#ae81ff">0</span>  NVIDIA GeForce RTX <span style="color:#ae81ff">3060</span>        On  | 00000000:00:10.0 Off |                  N/A |
</span></span><span style="display:flex;"><span>|  0%   48C    P2              46W / 170W |  11651MiB / 12288MiB |      0%      Default |
</span></span><span style="display:flex;"><span>|                                         |                      |                  N/A |
</span></span><span style="display:flex;"><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style="display:flex;"><span>                                                                                         
</span></span><span style="display:flex;"><span>+---------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| Processes:                                                                            |
</span></span><span style="display:flex;"><span>|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
</span></span><span style="display:flex;"><span>|        ID   ID                                                             Usage      |
</span></span><span style="display:flex;"><span>|<span style="color:#f92672">=======================================================================================</span>|
</span></span><span style="display:flex;"><span>|  No running processes found                                                           |
</span></span><span style="display:flex;"><span>+---------------------------------------------------------------------------------------+
</span></span></code></pre></div><p>And also, once you&rsquo;ve restarted your K3S agent service, check that it also picked up the change:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~ ‚ùØ sudo grep nvidia /var/lib/rancher/k3s/agent/etc/containerd/config.toml              octopusx@k8s12
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.<span style="color:#e6db74">&#34;nvidia&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.<span style="color:#e6db74">&#34;nvidia&#34;</span>.options<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>  BinaryName <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/usr/bin/nvidia-container-runtime&#34;</span>
</span></span></code></pre></div><h1 id="the-deployment">The Deployment</h1>
<p>In order for any container to take advantage of the nvidia containerd runtime plugin we need to create a special runtime class object, which can be referenced inside our kubernetes deployment yaml.</p>
<p>It looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">node.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RuntimeClass</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nvidia</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">handler</span>: <span style="color:#ae81ff">nvidia</span>
</span></span></code></pre></div><p>Since (at least in my case) I am running a single GPU that is attached to a specific K8S worker VM, we need to somehow force the workloads that require access to said GPU to be deployed on that specific node. To do this we create a node label that we can then reference in the pod affinity specification:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl label nodes k8s12 gpu<span style="color:#f92672">=</span><span style="color:#ae81ff">3060</span>
</span></span></code></pre></div><p>We can check that it worked by invoking the describe command on the K8S node object:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~/ ‚ùØ kubectl describe node k8s12                                                                                                                       ‚óã teleport-k3s
</span></span><span style="display:flex;"><span>Name:               k8s12
</span></span><span style="display:flex;"><span>Roles:              worker
</span></span><span style="display:flex;"><span>Labels:             beta.kubernetes.io/arch<span style="color:#f92672">=</span>amd64
</span></span><span style="display:flex;"><span>                    beta.kubernetes.io/instance-type<span style="color:#f92672">=</span>k3s
</span></span><span style="display:flex;"><span>                    beta.kubernetes.io/os<span style="color:#f92672">=</span>linux
</span></span><span style="display:flex;"><span>                    gpu<span style="color:#f92672">=</span><span style="color:#ae81ff">3060</span>
</span></span><span style="display:flex;"><span>                    kubernetes.io/arch<span style="color:#f92672">=</span>amd64
</span></span><span style="display:flex;"><span>                    kubernetes.io/hostname<span style="color:#f92672">=</span>k8s12
</span></span><span style="display:flex;"><span>                    kubernetes.io/os<span style="color:#f92672">=</span>linux
</span></span><span style="display:flex;"><span>                    kubernetes.io/role<span style="color:#f92672">=</span>worker
</span></span><span style="display:flex;"><span>                    node.kubernetes.io/instance-type<span style="color:#f92672">=</span>k3s
</span></span></code></pre></div><p>You will see that I created a label with a non-binary value. This is to make this system a little more future-proof on my side. This way I can match either on the label just &ldquo;existing&rdquo;, so if I want any GPU present to bind a deployment onto, or to a specific value if I want to assign a workload to a predefined model. Neat.</p>
<p>To make sure that our assignment mechanism works, we can spawn a test GPU benchmark container. If we did everything right this should return us a benchmark score for our GPU:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nbody-gpu-benchmark</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">restartPolicy</span>: <span style="color:#ae81ff">OnFailure</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">runtimeClassName</span>: <span style="color:#ae81ff">nvidia</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cuda-container</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nvcr.io/nvidia/k8s/cuda-sample:nbody</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;nbody&#34;</span>, <span style="color:#e6db74">&#34;-gpu&#34;</span>, <span style="color:#e6db74">&#34;-benchmark&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">NVIDIA_VISIBLE_DEVICES</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">value</span>: <span style="color:#ae81ff">all</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">NVIDIA_DRIVER_CAPABILITIES</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">value</span>: <span style="color:#ae81ff">all</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">affinity</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">nodeAffinity</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">nodeSelectorTerms</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">matchExpressions</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">gpu</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">Exists</span>
</span></span></code></pre></div><h3 id="refactai-chart">RefactAI Chart</h3>
<p>Now that we have checked that everything necessary to deploy refact is in place, we deploy it. Here is an example output of <code>helm template .</code> command for the helm chart that I created:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># Source: refact/templates/service.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">helm.sh/chart</span>: <span style="color:#ae81ff">refact-0.1.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/managed-by</span>: <span style="color:#ae81ff">Helm</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">ClusterIP</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">8008</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Source: refact/templates/deployment.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">helm.sh/chart</span>: <span style="color:#ae81ff">refact-0.1.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/managed-by</span>: <span style="color:#ae81ff">Helm</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">revisionHistoryLimit</span>: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">hostNetwork</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">runtimeClassName</span>: <span style="color:#ae81ff">nvidia</span> <span style="color:#75715e"># IMPORTANT!</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">dnsPolicy</span>: <span style="color:#ae81ff">ClusterFirstWithHostNet</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">enableServiceLinks</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">image</span>: <span style="color:#e6db74">&#34;smallcloud/refact_self_hosting:latest&#34;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">imagePullPolicy</span>: <span style="color:#ae81ff">IfNotPresent</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">securityContext</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">privileged</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;TZ&#34;</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;UTC+02:00&#34;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">8008</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">data</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/perm_storage</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">affinity</span>: <span style="color:#75715e"># This is how we tell it to only spawn on the node with our GPU</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">nodeAffinity</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">nodeSelectorTerms</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">matchExpressions</span>:
</span></span><span style="display:flex;"><span>              - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">gpu</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">Exists</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">data</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">persistentVolumeClaim</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Source: refact/templates/ingress.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">helm.sh/chart</span>: <span style="color:#ae81ff">refact-0.1.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/managed-by</span>: <span style="color:#ae81ff">Helm</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#75715e"># insert your own ingress class to use</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># insert any other annotations you need for your ingress controller</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tls</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#ae81ff">refact.example.com</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">secretName</span>: <span style="color:#e6db74">&#34;wildcard-cert&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">refact.example.com</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">http</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">path</span>: <span style="color:#e6db74">&#34;/&#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">service</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">port</span>:
</span></span><span style="display:flex;"><span>                  <span style="color:#f92672">number</span>: <span style="color:#ae81ff">8008</span>
</span></span></code></pre></div><p>For me an extra step here would be to add <code>refact.example.com</code> to my local DNS to expose this ingress entry (I just add it to my PiHole server).
If all goes well, you should be able to go to <code>refact.example.com</code> (taking that you have DNS set up for this) you should see a screen much like this:</p>
<p><img src="refact_web.png" alt="refact web UI"></p>
<p>Here I have loaded 2 small models, one that provides code completion and one that provides chat functionality.
Also, I have no idea why I am getting the VRAM warning message. It has been there for me since the beginning no matter what model I select and since everything is working for me I am going to assume it&rsquo;s a UI bug&hellip;</p>
<h3 id="vscode-plugin">VSCode Plugin</h3>
<p>Let&rsquo;s finally go harness the power of the AI in a practical manner. First, we install the refact.ai plugin:</p>
<p><img src="refact_vscode_plugin.png" alt="refact vscode plugin"></p>
<p>Then we go to its config page to enter our local instance URL:</p>
<p><img src="plugin_config.png" alt="plugin config"></p>
<p>And off you go! Refact should now automatically start generating suggestions in your VSCode editor window as you type:</p>
<p><img src="autocompletion.png" alt="autocompletion"></p>
<p>In my case, it will also allow you to chat with the llama7b model:</p>
<p><img src="llama7b_chat.png" alt="llama7b chat"></p>
<h2 id="the-end">The End</h2>
<p>All in all, I am quite happy with this setup. It was very straight forward and pretty much just worked. I may even sign up for their paid version just to support their project, well done refact.ai! I haven&rsquo;t used this in practice much so I don&rsquo;t feel comfortable giving a verdict on how good the small models that I am able to fit into my 3060&rsquo;s memory are. However what I can say is that the models execute extremely fast on the RTX 3060 12GB gpu&hellip; The code generation is very fast and the llama7b models chat is pretty much instant. Much more responsive than my previous attempt at CPU hosted model and downright impressive, with such a relatively small investment in hardware. Something I really wish I was able to do is expose my llama7b model hosted by refact in other apps via an openAI API endpoint. Another would be to be able to host a single larger model that does both code completion as well as chatting. That would make me 120% happy&hellip; For the moment, I have browsed the refact&rsquo;s github issue tracker as well as their discord server (<a href="https://www.smallcloud.ai/discord">https://www.smallcloud.ai/discord</a>) which seems to be active and will see if I can find out more about the project&rsquo;s roadmap and if the few functionalities that are missing will be added over time. Hope you enjoyed this tutorial and will come back again when the next entry rolls in.</p>
]]></content>
        </item>
        
        <item>
            <title>Reasonably Large Language Models</title>
            <link>https://blog.octopusx.de/posts/reasonably_large_language_models/</link>
            <pubDate>Mon, 23 Oct 2023 20:20:24 +0200</pubDate>
            
            <guid>https://blog.octopusx.de/posts/reasonably_large_language_models/</guid>
            <description>There&amp;rsquo;s no such thing as free lunch right? I think that openAI are learning this hard way as they are burning all kinds of cash and are no longer providing me with free tokens to use. I must say, I quite liked using CodeGPT in VSCode. Thankfully we have had a bunch of developments lately in the realm of self hosted local AI initiatives, following Facebook&amp;rsquo;s Llama2 release. At the same time more and more applications have plugins that lets them interact with openAI API compatible local LLMs, including VSCode!</description>
            <content type="html"><![CDATA[<p>There&rsquo;s no such thing as free lunch right? I think that openAI are learning this hard way as they are burning all kinds of cash and are no longer providing me with free tokens to use. I must say, I quite liked using CodeGPT in VSCode. Thankfully we have had a bunch of developments lately in the realm of self hosted local AI initiatives, following Facebook&rsquo;s Llama2 release. At the same time more and more applications have plugins that lets them interact with openAI API compatible local LLMs, including VSCode! So, let&rsquo;s dive in!</p>
<p><strong>Ask Llama!</strong></p>
<blockquote>
<p>Tell me a joke about large language models</p>
</blockquote>
<blockquote>
<p>Why did the large language model go to therapy? Because it had a lot of issues with its input data.</p>
</blockquote>
<h1 id="tldr">TLDR;</h1>
<p>I am pretty new to this whole AI thing, so we will split this into a few sections and list what worked for me and what didn&rsquo;t. K8S deployment process first, the basics of what models are available and which ones worked best for me, a few simple integration and what we are going to cover in the future. I got most of what I wanted to work, but to unlock the full potential I need to try this on a GPU&hellip;</p>
<h2 id="k8s-deployment">K8S Deployment</h2>
<p><strong>Ask Llama!</strong></p>
<blockquote>
<p>What is Kubernetes?</p>
</blockquote>
<blockquote>
<p>Kubernetes is an open-source container orchestration system that automates the deployment, scaling, and management of containerized applications. It helps to manage complex distributed systems and enables developers to focus on writing code instead of managing infrastructure.</p>
</blockquote>
<p>The first well described github repository I have found that explains how to easily host openAI compatible API for an LLM was Umbrel&rsquo;s: <a href="https://github.com/getumbrel/llama-gpt">https://github.com/getumbrel/llama-gpt</a>. They didn&rsquo;t just provide examples and docker-compose files, but even a simple set of Kubernetes templates, which was a nice kick start for me. A few minutes of digging through that and we had a ready helm chart based on their templates. They actually built a slick little web UI that mimic chat GPT&rsquo;s, very handy! What hasn&rsquo;t worked for me though is their mechanism for downloading various models. To load a model into the containerized environment I first created a k8s volume manually, mounted it to a VS Code web server where I used the terminal to download all the models I wanted to use. The container that provides the openAI API is from the llama-cpp-python binding project found here: <a href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a>.</p>
<p>The second project I found was definitely more accomplished and well maintained: the <a href="localai.io">localai.io</a> project. Where the Umbrel setup uses python c++ bindings, this one implements its own in golang: <a href="https://github.com/go-skynet/LocalAI">https://github.com/go-skynet/LocalAI</a>. It does not provide any form of web UI add on, however, unlike the Umbrel repo, this one includes working helm charts, instructions for K8S deployment and a working model download mechanism! All you have to do is add URLs to a list in helm chart&rsquo;s <code>values.yaml</code> file and an init container will take care of it. There is even a flag to force re-download any model that already exists on the volume in case that is necessary.</p>
<p>I ended up running both of them in my cluster to test and play with, and will likely continue to do so going forward. Having deployed and briefly used both of the setups I realized quickly that they are fairly slow on my current hardware, which means that if I can have 2 different setups with different models at the same time I can avoid long request queueing. Additionally it is worth noting that if you wish to use these LLM containers with multiple models, each model used will be loaded into memory, so you might end up with a container that consumes upwards of 16 GB of RAM. Going forward I decided to use the Umbrel project to host my chat-focused model and LocalAI.io for my code generation one, which means that I have 2 independent services serving distinct functions and 2 separate pods, each of which will use around 8GB of RAM and can run on a different node in my K8S cluster.</p>
<p>As I mentioned earlier, the <code>llama-cpp-python</code> base configured in Umbrel&rsquo;s repo doesn&rsquo;t come with a proper helm chart, however I will make mine public once I find the time to tidy it up and push it to Github. Stay tuned for that if you&rsquo;re interested!</p>
<h2 id="models">Models</h2>
<p><strong>Ask Llama!</strong></p>
<blockquote>
<p>What is a large language model</p>
</blockquote>
<blockquote>
<p>A large language model is an advanced artificial intelligence system designed to generate human-like text, understand natural language, and perform tasks such as translation, summarization, and question answering by analyzing vast amounts of data.</p>
</blockquote>
<p>So, the APIs are not of very much use to anybody if you don&rsquo;t give them a model to work with. Me being super new to all this, I started my research from the Umbrel&rsquo;s <a href="https://github.com/getumbrel/llama-gpt/blob/master/README.md">llama-gpt repo&rsquo;s readme</a>.
Handily, it did contain a nice table summarizing the models compatible with this setup:
<img src="models_table.png" alt="llama-cpp-python tested models"></p>
<p>What it however does not tell you, at least not explicitly, is what each of these models does or where to get them. I do understand that the <code>B</code> value stands for billions of parameters. The more parameters, the more accurate model, but the more memory and computational resources it needs to run (warning: vast oversimplification). If we dig a little and take a sneak peak at the docker-compose.yaml files in the repo, we will see the following in line 15:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">(...)</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">MODEL_DOWNLOAD_URL</span>: <span style="color:#e6db74">&#39;${MODEL_DOWNLOAD_URL:-https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGML/resolve/main/nous-hermes-llama-2-7b.ggmlv3.q4_0.bin}&#39;</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">(...)</span>
</span></span></code></pre></div><p>Aha! This is where they are getting the models from. Short duck duck go later, and we find out that huggingface.co is a fantastic repository for all kinds of AI models, ready to be downloaded for free. I recommend reading the repositories that you can find there, as they describe often in detail what each model does, what it is good at and what requirements it has. The key thing I learned is this:</p>
<ul>
<li>Models marked with <code>GGML</code> are an older standard, single-file packaged models that can run on a CPU</li>
<li>Models marked with <code>GGUF</code> are meant to be a more versatile, improved version of GGML packaging for ML models</li>
</ul>
<p>There are a lot of different parameters that each model is pre-configured with, but from reading the info in the repos, models marked <code>q4_0</code> give the best balance of speed and accuracy for a given parameter size (usually).</p>
<p>Yes, this was a quick search and a bunch of trial and error, so if you want to explain this to me or know of a cool resource online that I could read or watch to understand these parameters better, hit me up on mastodon!</p>
<p>From my side, I also have to give a shout out to <a href="https://gpt4all.io/">https://gpt4all.io/</a>, as it also has a lot of useful information about the relative qualities and purpose of each model.</p>
<p>Now, to set out some expectations and context, I am running these LLMs in my K8S cluster on CPUs. My home lab has 3 nodes, each spotting a Ryzen 3000 CPU. This means AVX1 and AVX2 but not AVX512. For the uninitiated, those are vector instruction set extensions which help accelerate some of the AI tasks. On each of these boxes I have a VM with CPU passthrough configured to make sure the workloads see the extensions, and each with 6 virtual CPU cores (6 threads essentially).</p>
<h3 id="chat-models">Chat Models</h3>
<p>In the couple of days that I played with all of these, there were 2 models that worked reasonably well on my machines:</p>
<ul>
<li><a href="https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML">llama-2-13b-chat.ggmlv3.q4_0</a></li>
<li><a href="https://huggingface.co/TheBloke/Nous-Hermes-Llama2-GGML">nous-hermes-llama2-13b.ggmlv3.q4_0</a></li>
</ul>
<p>Each of those models are about 8GB in size (i.e. will take 8GB of RAM once loaded up) and will run reasonably happily from within the <code>llama-cpp-python</code> container. Most of the queries I asked these models returned quickly enough to be considered &ldquo;interactive&rdquo;.</p>
<p>Pro tip. When you configure <code>llama-cpp-python</code>, make sure to set the <code>n_threads</code> environment variable to the number of threads you want to make available to the model for computing. All of the parameters available can be found in the <a href="https://abetlen.github.io/llama-cpp-python/#api-reference">official documentation</a>.</p>
<h3 id="coding-models">Coding Models</h3>
<p>I had very high hopes for the coding models I have to say, however I found most of them temperamental and tricky to work with. For some reason a lot of them, especially the <code>codellama</code> designated ones, were falling into a loop of repeated output and would generate the same answer over and over before I manually terminated them. I have not found a definitive answer online what causes this behavior and was not able to make them work with the VSCode integrations that I used. This left me with the one model that has been the most consistently working as a coding assistant:</p>
<ul>
<li><a href="https://huggingface.co/nomic-ai/gpt4all-falcon-ggml/resolve/main/ggml-model-gpt4all-falcon-q4_0.bin">ggml-model-gpt4all-falcon-q4_0</a></li>
</ul>
<p>This model worked fine in the VSCode integrations that I tried with it, though bear in mind, it is not an instruct model. It is good if you need it to explain code for you or suggest code via a chat-like interaction. It will not however work for straight up code generation inside your editor. OpenAI API is apparently not designed for that and you need a dedicated instruct model and an instruct compatible API endpoint. And self hosted versions of such software and models require the use of a GPU which I currently do not have! We will have to explore this avenue at another occasion then&hellip; More details on code generation in the integrations section.</p>
<h2 id="integrations">Integrations</h2>
<h3 id="umbrel-llama-ui">Umbrel Llama UI</h3>
<p>Ok, so we have ourselves a web API deployed, we downloaded our models. It is finally time to use this stuff! The easiest way to start in my opinion is the Umbrel Llama UI that we can get from <a href="https://github.com/getumbrel/llama-gpt">here</a>. This creates a ChatGPT-like web UI for you that you can use to create threads of conversations with different basic parameters, such as the system prompt and temperature.</p>
<p><img src="umbrel_web_ui_new.png" alt="ubmrel llama web ui - new chat"></p>
<p>Just as an FYI:</p>
<ul>
<li>Temperature: setting on a scale of 0.0 to 1.0, that governs how factual and to the point answers will the model try to give you (0.0) vs how creative you want it to be (1.0), where setting 0.5 could be described as neutral.</li>
<li>System prompt: information you can feed the model that sets the tone and the desired outcome of the conversation you are trying to have with the model. It can be things like: &ldquo;You are a helpful AI and will answer concisely&rdquo; or &ldquo;We are going to be role playing where I am Mario and you are princess Peach&rdquo;, you get the idea&hellip;
This is great for general chat usage but also to test that your models are working and performing.</li>
</ul>
<p><img src="umbrel_web_ui.png" alt="ubmrel llama web ui"></p>
<h3 id="nextcloud-assistant">Nextcloud Assistant</h3>
<p><strong>Ask Llama!</strong></p>
<blockquote>
<p>What is Nextcloud?</p>
</blockquote>
<blockquote>
<p>Nextcloud is a open-source, self-hosted file synchronization and sharing platform that allows users to access their files from multiple devices, share files with others, and collaborate on projects.</p>
</blockquote>
<p>Nextcloud has recently been making noise around their brand spanking new OpenAI integration being made available to use with self hosted LLMs. Being able to try this out was a fairly large motivator for me. Partly to see actually how easy it was to set up and partly because I found the features advertised, if not useful, then at least fun to try. So, once you have your own API endpoint ready, you can go to the administrator settings panel in your Nextcloud instance and enter it somewhere there&hellip; And by somewhere I mean somewhere, because for some reason it is in the <code>Connected Accounts</code> tab, and not the <code>Artificial Intelligence</code> tab, but what do I know. So, I enter it and&hellip; I get a green tick in the top right corner saying admin setting saved. That&rsquo;s good right?</p>
<p><img src="nc_ai_config.png" alt="nextcloud openai plugin configuration"></p>
<p>Well, all of the AI-related functionalities fail&hellip; And they fail after a long long time, which indicates some sort of a timeout issue maybe? I browsed through the logs and all I find this this&hellip;</p>
<pre tabindex="0"><code>OpenAI API error : Host violates local access rules
</code></pre><p>Which tells me nothing, and duck duck going it ain&rsquo;t helping much either.</p>
<p>A couple of days pass and I find this beautiful issue on the Github issue tracker for the Nextcloud OpenAI integration: <a href="https://github.com/nextcloud/integration_openai/issues/47">https://github.com/nextcloud/integration_openai/issues/47</a></p>
<p>Well, I had no idea I am supposed to be able to select the model there. The UI does not show that as an option, not even grayed out. As you see I chimed into the conversation, reporting similar behavior. Low and behold, I get a quick response from a dev and my issue is resolved!</p>
<p><img src="github_issue.png" alt="github issue response"></p>
<p>So, now I have two key takeaways from this interaction. One is, that the devs clearly know what they&rsquo;re doing and they were super quick to help me and fix my issue. Two, however, is a big problem in that I needed this help in the first place!</p>
<p>Basically, in order for Nextcloud to allow you to talk to a locally hosted LLM you need to change this setting:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-php" data-lang="php"><span style="display:flex;"><span><span style="color:#e6db74">&#39;allow_local_remote_servers&#39;</span><span style="color:#f92672">=&gt;</span><span style="color:#66d9ef">true</span>
</span></span></code></pre></div><p>in the main PHP config file for Nextcloud. But (at least at the time of writing) this is not said anywhere on the plugin install page or in the documentation as far as I am aware. And it seems that this flag is set to <code>false</code> by default. Oh dear&hellip;</p>
<p>So, with that out of the way, now we know that you have set things up correctly once the model field <strong>magically reveals itself to us</strong> in the config UI.</p>
<p><img src="nc_ai_config_2.png" alt="working nextcloud local ai config"></p>
<p>We can get to work now! Let&rsquo;s generate some text!</p>
<p><img src="nc_free_prompt.png" alt="image of nextcloud AI free prompt trigger"></p>
<p>I press that magical sparkly &ldquo;Free Prompt&rdquo; button and&hellip; Poof! Nothing XD To cut to the chase here and not drag this out much longer, it seems that Nextcloud is trying to execute most of the calls to your local LLM endpoint asynchronously. Which is a good choice in my opinion. The problem for me however is that I am running Nextcloud in Kubernetes, and to facilitate job processing I had set up a web cron container, that basically gets spawned twice per second to pick up any currently queued jobs. This includes but is not limited to the OpenAI API calls. This makes it fairly unpredictable as to when your jobs will be processed, and though largely fine for strictly speaking background jobs, I was expecting my AI queries to complete faster than within an hour or two&hellip; The only call that I found so far to be computed synchronously (as in, while you wait) is the smart picker <code>ChatGPT-like text generation (with LocalAI)</code> option that I triggered inside a note that I was editing. There you get a &ldquo;preview&rdquo; button, which will lock the prompt popup until you get your result from the LLM API.</p>
<p><img src="smart_picker.png" alt="smart picker popup"></p>
<p>Sadly my experiences with the ready made prompts that run async are also rather bad, and your milage may also vary depending on which LLM you use. With <code>llama-2-13b-chat</code> for example I generally get gems like this:</p>
<p><img src="summarise.png" alt="no, you sumarise it yourself"></p>
<p>As far as I am aware there is no way to adjust the wording of the queries that is used behind these &ldquo;summarize/generate headline/reformulate&rdquo; buttons, so whether they will actually result in a useful output is up to luck. This is obviously only based on a few days of using this model and integration, so maybe I am just missing something&hellip;</p>
<p>In summary, with my current setup the local OpenAI API integration with Nextcloud is not very useful or practical. I feel like it lacks basic functionality such as the inability to set model temperature or system prompt, as well as being able to choose if a command is executed by the LLM synchronously or asynchronously. I am relatively positive that Nextcloud will iron out this integration and make it more configurable over time so I will happily revisit it at a later date. Though I really wish that they would come up with some other way to run jobs for Kubernetes deployments altogether, other than web cron&hellip;</p>
<h3 id="visual-studio-code">Visual Studio Code</h3>
<p><strong>Ask Llama!</strong></p>
<blockquote>
<p>What is Visual Studio Code?</p>
</blockquote>
<blockquote>
<p>Visual Studio Code is a free, cross-platform source-code editor that runs on Windows, Linux, and macOS. It includes support for debugging, task running, and Git control, among other features. It is popular among developers for its speed, flexibility, and extensibility.</p>
</blockquote>
<p>Now we are getting to where it&rsquo;s at for productivity tasks! At least for my use case. AI assisted programming was something that I was already familiar with by the virtue of using the CodeGPT plugin for VSCode. I wouldn&rsquo;t call myself a power user but I certainly found it useful at more than one occasion and wanted to find a self-hosted alternative that will respect my data and not charge me a subscription!</p>
<p>In the end I found and tested two extensions which can be used with a local OpenAI API endpoint, <a href="https://github.com/nvms/wingman">Wingman</a> and <a href="https://github.com/continuedev/continue">Continue</a>.</p>
<h4 id="wingman">Wingman</h4>
<p>Wingman is the first VSCode extension that I found and I really like the project. What it essentially is, is a set of pre-configured but user customizable queries that you can ask your LLM, with a mechanism to automatically paste the responses into the file you&rsquo;re editing.</p>
<p><img src="wingman.png" alt="wingman view"></p>
<p>You can select actions of various categories, such as completion, testing, refactoring or just chat. Where I think the real power comes from here is the customizability. You can add your own prompts in the json config file, here&rsquo;s the example it comes with:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;wingman.userCommands&#34;</span><span style="color:#960050;background-color:#1e0010">:</span> [
</span></span><span style="display:flex;"><span>        {
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;command&#34;</span>: <span style="color:#e6db74">&#34;exImprove&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;label&#34;</span>: <span style="color:#e6db74">&#34;Suggest improvements&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;userMessageTemplate&#34;</span>: <span style="color:#e6db74">&#34;I have the following {{language}} code:\n```{{filetype}}\n{{text_selection}}\n```\nPlease provide a list of suggestions for improving the code in markdown format. Consider aspects such as readability, performance, maintainability, and best coding practices. Explain each suggestion briefly, highlighting the specific areas that could be enhanced. Your suggestions should aim to improve the code without changing its behavior.&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;systemMessageTemplate&#34;</span>: <span style="color:#e6db74">&#34;You are an assistant to a {{language}} developer.&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;callbackType&#34;</span>: <span style="color:#e6db74">&#34;none&#34;</span>,
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">&#34;category&#34;</span>: <span style="color:#e6db74">&#34;My prompts&#34;</span>
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    ]
</span></span></code></pre></div><p>This article is sadly not going to cover how well most of the features work. This is because most of the more interesting commands that I ran via Wingman (or Continue for that matter), resulted in my server CPUs spinning away and nothing else.</p>
<p><img src="spinning.png" alt="cpu utilization during coding"></p>
<p>I was still able to do simple things, such as asking the LLM to generate comments for simple code blocks, or use the built in chat to ask programming-specific questions.</p>
<h4 id="continue">Continue</h4>
<p>Continue is another alternative which is very close in functionality to Wingman, with a few subtle differences. For a start, Continue is more keyboard driven. You&rsquo;re not selecting query types from a menu but rather type <code>/</code> followed by the type of command you want to run. This also includes code editing, generating tests, comments, etc.</p>
<p><img src="continue_ui.png" alt="continue UI"></p>
<p>You can also configure multiple backends simultaneously and switch between them from the UI, which is very useful if you want to split your use between a local and a public cloud API or just when you&rsquo;re testing multiple models, maybe specific for different programming languages&hellip;</p>
<p>Also, where Wingman basically overrides the code you pasted into it when you ask it to modify it, Continue creates you a visual diff, which you can either accept, reject or ask for adjustments to. Very neat, and which one you prefer depends on your style of working with code and coding editors.</p>
<p>With Continue you can also simply add code to the context of specific queries by selecting it and pressing <code>ctrl+m</code>. A visual queue will appear that the selected block is attached and you can remove it at any point. All of your chat history and context is also logged for you to browse through later which I definitely like too.</p>
<p><img src="continue_ui_generation.png" alt="continue generation"></p>
<p>The models you want to use can be easily configured via modifying a config.py residing in your home dir (<code>~/.continue/config.py</code>). Here&rsquo;s my block for the 3 models available on my local-ai instance:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span><span style="display:flex;"><span>    models<span style="color:#f92672">=</span>Models(
</span></span><span style="display:flex;"><span>		saved<span style="color:#f92672">=</span>[OpenAI(
</span></span><span style="display:flex;"><span>			model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;nous-hermes-llama2-13b.ggmlv3.q4_0.bin&#34;</span>,
</span></span><span style="display:flex;"><span>			api_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>			api_base<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://local-ai.octopusx.de&#34;</span>
</span></span><span style="display:flex;"><span>		),
</span></span><span style="display:flex;"><span>		OpenAI(
</span></span><span style="display:flex;"><span>			model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;starcoder.ggmlv3.q4_0.bin&#34;</span>,
</span></span><span style="display:flex;"><span>			system_message<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;You are a friendly AI that is helping me write high quality Python code. Give short and concrete answers.&#34;</span>,
</span></span><span style="display:flex;"><span>			api_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>			api_base<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://local-ai.octopusx.de&#34;</span>
</span></span><span style="display:flex;"><span>		)],
</span></span><span style="display:flex;"><span>		default<span style="color:#f92672">=</span>OpenAI(
</span></span><span style="display:flex;"><span>			model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ggml-model-gpt4all-falcon-q4_0.bin&#34;</span>,
</span></span><span style="display:flex;"><span>			system_message<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;You are a friendly AI that is helping me write high quality Python code. Give short and concrete answers.&#34;</span>,
</span></span><span style="display:flex;"><span>			api_key<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>			api_base<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;https://local-ai.octopusx.de&#34;</span>
</span></span><span style="display:flex;"><span>		)
</span></span><span style="display:flex;"><span>	),
</span></span><span style="display:flex;"><span><span style="color:#f92672">...</span>
</span></span></code></pre></div><p>Sadly again, due to my current hardware limitations, I could not use this extension very much beyond asking it to generate me some simple example code. The same CPU pegging, timeout inducing computation of doom got in my way.</p>
<h2 id="whats-next">What&rsquo;s next</h2>
<p>Being mostly hardware restricted in most of my scenarios I think we will have to try GPU-based generation next. Though that will have to wait for another blog post, as I don&rsquo;t even own a single dedicated GPU!</p>
<p>There are obviously many more extensions and integrations to explore now that I have this whole deployment going, the least of which would be making the Llama 2 chat bot available via chat rooms on my Matrix server. Exciting times.</p>
<p>So, stay tuned for future shenanigans in the land of generative self hosted AI models!</p>
<p><strong>Ask Llama!</strong></p>
<blockquote>
<p>Generate me an outro section for my technical blog post about LLMs, inviting the reader to come back to the site for the fantastic content in the future.</p>
</blockquote>
<blockquote>
<p>Thank you for reading my blog post on large language models and their uses in self-hosted infrastructure. Be sure to check back for more exciting content and insights into the world of AI and natural language processing. Until next time!</p>
</blockquote>
]]></content>
        </item>
        
        <item>
            <title>Nextcloud Conference 2023</title>
            <link>https://blog.octopusx.de/posts/nc_conf_2023/</link>
            <pubDate>Thu, 05 Oct 2023 09:23:35 +0200</pubDate>
            
            <guid>https://blog.octopusx.de/posts/nc_conf_2023/</guid>
            <description>Although I do a lot of semi-public speaking in a professional capacity, I have recently given my first talk at a conference for fun! I use nextcloud as the backbone of my open source, privacy focused self hosting infastructure, the WebDav/CalDav integrations really enable my family to largly de-google their smarphone experience. Happy to give something back to a project I sincerely respect. This short lightning talk was based on my previous blog post about how I designed my backup and restore procedures.</description>
            <content type="html"><![CDATA[<p>Although I do a lot of semi-public speaking in a professional capacity, I have recently given my first talk at a conference for fun! I use nextcloud as the backbone of my open source, privacy focused self hosting infastructure, the WebDav/CalDav integrations really enable my family to largly de-google their smarphone experience. Happy to give something back to a project I sincerely respect. This short lightning talk was based on my previous blog post about <a href="../backup_restore/">how I designed my backup and restore procedures</a>. Please, enjoy the video and definitely check out the blog entry for the details of how it&rsquo;s done.</p>

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="https://www.youtube.com/embed/qUFh8sDg7-k" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="Homelab disaster recovery made easy with open source tools! | Nextcloud Conference 2023"></iframe>
</div>

]]></content>
        </item>
        
        <item>
            <title>Encrypted Volumes</title>
            <link>https://blog.octopusx.de/posts/encrypted_volumes/</link>
            <pubDate>Mon, 02 Oct 2023 23:38:00 +0200</pubDate>
            
            <guid>https://blog.octopusx.de/posts/encrypted_volumes/</guid>
            <description>When I started self hosting services I was afraid of storing my non-sensitive data encrypted. As I played with different kinds of hardware setups, hypervisors and hosting strategies I have made a decision to not encrypt the volumes attatched to my docker containers. This was so that when I inevitably break stuff, I can easily still access everything and move it to a new home where it can be used again.</description>
            <content type="html"><![CDATA[<p>When I started self hosting services I was afraid of storing my non-sensitive data encrypted. As I played with different kinds of hardware setups, hypervisors and hosting strategies I have made a decision to not encrypt the volumes attatched to my docker containers. This was so that when I inevitably break stuff, I can easily still access everything and move it to a new home where it can be used again.</p>
<p>After many iterations and failed attempts at having a sensible strategy for storing the data of my stateful applications I ended up sticking with Longhorn, which has proven to be reliable and flexible yet simple enough to fit my needs. For more information on how I use it I recommend checking out my previous posts: <a href="https://blog.octopusx.de/posts/backup_restore/">https://blog.octopusx.de/posts/backup_restore/</a> and <a href="https://blog.octopusx.de/posts/homelab_ha_edition/">https://blog.octopusx.de/posts/homelab_ha_edition/</a>.</p>
<h2 id="building-trust">Building trust</h2>
<p>Having been using Longhorn now for a good few years, surviving a few cluster and Longhorn version upgrades I decided it is time to start encrypting my volumes again. This is particularly important now that I have an offsite backup. Thankfully Longorn has a built in encryption mechanism that can be used to decrypt volumes on the fly while mounting them to their respective containers for use.</p>
<h2 id="the-setup">The setup</h2>
<p>We will be following Longhorn&rsquo;s official guide to set this up: <a href="https://longhorn.io/docs/1.5.1/advanced-resources/security/volume-encryption/">https://longhorn.io/docs/1.5.1/advanced-resources/security/volume-encryption/</a></p>
<p>It states that we need the <code>dm_crypt</code> kernel module loaded and <code>cryptsetup</code> installed on the worker nodes.
I can see that I have cryptsetup on my Ubuntu 22.04 server workers:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~ ‚ùØ sudo apt search cryptsetup                                                                                    5s user@worker01
</span></span><span style="display:flex;"><span>Sorting... Done
</span></span><span style="display:flex;"><span>Full Text Search... Done
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>cryptsetup/jammy-updates,now 2:2.4.3-1ubuntu1.1 amd64 <span style="color:#f92672">[</span>installed,automatic<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>  disk encryption support - startup scripts
</span></span></code></pre></div><p>However the dm-crypt kernel module is nowhere to be seen. Since cryptsetup seems to be working fine though, let&rsquo;s see if it is indeed needed or if it is a soft requirement&hellip;</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>lsmod | rg crypt
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (sound of crickets, but in text form...)</span>
</span></span></code></pre></div><p>First thing we want to create is the secret that will be used by longhorn to encrypt and decrypt all the volumes. It has to look like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Secret</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">longhorn-crypto</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">longhorn-system</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">stringData</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">CRYPTO_KEY_VALUE</span>: <span style="color:#e6db74">&#34;super-secret-extra-long-key&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">CRYPTO_KEY_PROVIDER</span>: <span style="color:#e6db74">&#34;secret&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">CRYPTO_KEY_CIPHER</span>: <span style="color:#e6db74">&#34;aes-xts-plain64&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">CRYPTO_KEY_HASH</span>: <span style="color:#e6db74">&#34;sha256&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">CRYPTO_KEY_SIZE</span>: <span style="color:#e6db74">&#34;256&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">CRYPTO_PBKDF</span>: <span style="color:#e6db74">&#34;argon2i&#34;</span>
</span></span></code></pre></div><p>We want to create this secret manually by the means of running <code>kubectl apply -f &lt;filename&gt;</code>, although I recommend storing the <code>CRYPTO_KEY_VALUE</code> in a password manager as a backup. I was wandering as to the length of this key too but didn&rsquo;t imediately find any recommendations, so I chose a 40 character random string for now.</p>
<p>Also, bear in mind, that whenever you restore an encrypted volume from a backup to a new cluster you will need to re-create this secret, otherwise the data on your volumes won&rsquo;t be readable. I have created this secret on both my production cluster as well as my backup cluster already to enable me to test the backup/restore procedure.</p>
<p>Next we need to create a new storage class, which we will use when creating encrypted volumes, new and restored from a backup alike:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">StorageClass</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">storage.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">longhorn-crypto-global</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">provisioner</span>: <span style="color:#ae81ff">driver.longhorn.io</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">allowVolumeExpansion</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">parameters</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">numberOfReplicas</span>: <span style="color:#e6db74">&#34;2&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">staleReplicaTimeout</span>: <span style="color:#e6db74">&#34;2880&#34;</span> <span style="color:#75715e"># 48 hours in minutes</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">fromBackup</span>: <span style="color:#e6db74">&#34;&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">encrypted</span>: <span style="color:#e6db74">&#34;true&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># global secret that contains the encryption key that will be used for all volumes</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">csi.storage.k8s.io/provisioner-secret-name</span>: <span style="color:#e6db74">&#34;longhorn-crypto&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">csi.storage.k8s.io/provisioner-secret-namespace</span>: <span style="color:#e6db74">&#34;longhorn-system&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">csi.storage.k8s.io/node-publish-secret-name</span>: <span style="color:#e6db74">&#34;longhorn-crypto&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">csi.storage.k8s.io/node-publish-secret-namespace</span>: <span style="color:#e6db74">&#34;longhorn-system&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">csi.storage.k8s.io/node-stage-secret-name</span>: <span style="color:#e6db74">&#34;longhorn-crypto&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">csi.storage.k8s.io/node-stage-secret-namespace</span>: <span style="color:#e6db74">&#34;longhorn-system&#34;</span>
</span></span></code></pre></div><p>You can see in the above example that they refer to our secret name. This way of creating volumes means that they all are encrypted with the same key. If you want to have a different key for each volume, the official Longhorn guide tells you how to do that too.</p>
<h2 id="conversion-to-encrypted-volumes">Conversion to encrypted volumes</h2>
<p>There is no automated way to convert an unencrypted volume into an encrypted one in Longhorn as far as I know. In order to achieve this we will have to resort to a few tricks that will allow us to migrate the data from the old unencrypted volumes to the new encrypted ones.</p>
<p>The first thing I tried, thinking that I am cheeky, was to first make a backup of the Longhorn volume that I wish to have encrypted, then restore that backup with the &ldquo;encrypted&rdquo; flag selected. This however resulted in errors when trying to mount such a volume. It was worth a shot though, it would make our life super easy if this worked!</p>
<p>So, off to go be more tricksy. Whenever I need to perform any manipulation on data inside my K8S cluster volumes, I usually temporarily attatch said volume to my VSCode server container. This gives me a convenient way to move data around using the built-in VSCode terminal and edit any files then and there if necessary. This turned out the be the way to go.</p>
<h3 id="create-a-backup">Create a backup</h3>
<p>First I would go and create an ad-hoc backup of the volume I am trying to migrate. This way I can migrate the current state of the data on each volume without interrupting the opration of the application using said volume.</p>
<p><img src="create_backup.png" alt="Create New Manual Backup"></p>
<h3 id="restore-to-a-new-unencrypted-volume">Restore to a new unencrypted volume</h3>
<p>Now that we have fresh data in our backup system, we can go ahead and use the restore feature while creating a brand new unencrypted volume. This will allow us to have a live copy of the latest data that we can subsequently mount to our VSCode container, again, without interupting the related application.</p>
<p><img src="restore_backup.png" alt="Create New Volume from Backup"></p>
<h3 id="create-a-new-encrypted-volume">Create a new encrypted volume</h3>
<p>Then I go ahead and create a new encrypted volume. This will be the target volume that we will be eventually attatching to the container once all the data is migrated.</p>
<p><img src="create_encrypted.png" alt="Create New Encrypted Volume"></p>
<h3 id="attatch-the-new-volumes-to-vscode">Attatch the new volumes to VSCode</h3>
<p>In order to attatch the 2 volumes we just crated they first have to have PVs and PVCs created. This does not automatically happen when you create a new volume either fresh of from a backup. That is unless you&rsquo;re restoring a backup and select the &ldquo;use previous PVC name&rdquo;, which we do not want to do because that PVC still exists and is attatched to the original container.</p>
<p><img src="pvc.png" alt="Create PVCs"></p>
<p>Once you create the PVCs we go to the VSCode helm chart and modify it, adding the extra two volume mounts.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">extraVolumeMounts</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Source volume, restored from backup</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dev-01</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/mnt/gitea-mariadb-02</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">existingClaim</span>: <span style="color:#ae81ff">gitea-mariadb-02</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">readOnly</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># Destination encrypted volume, freshly created</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dev-02</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/mnt/gitea-mariadb-04</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">existingClaim</span>: <span style="color:#ae81ff">gitea-mariadb-04</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">readOnly</span>: <span style="color:#66d9ef">false</span>
</span></span></code></pre></div><p>Then apply the changes so that they take effect:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">helm upgrade vscode .</span>
</span></span></code></pre></div><h3 id="data-sync">Data sync</h3>
<p>Our vanilla VSCode container is missing <code>rsync</code>, which is easily remedied. <code>CTRL</code>+<code>j</code> brings up the terminal and off we go:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt update
</span></span><span style="display:flex;"><span>sudo apt install rsync
</span></span></code></pre></div><p>Just like that:</p>
<p><img src="vscode.png" alt="Use VSCode"></p>
<p>Once that is done, run a sync command to copy all of our data from the backup volume and into the new encrypted volume:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo rsync -avxHAX --info<span style="color:#f92672">=</span>progress2 --info<span style="color:#f92672">=</span>name0 /mnt/gitea-mariadb-02/* /mnt/gitea-mariadb-04
</span></span></code></pre></div><p>Once this is done you can do a quick sanity check, make sure that both the volumes have the same weight before moving on:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>du -sh /mnt/gitea-mariadb-02
</span></span><span style="display:flex;"><span>du -sh /mnt/gitea-mariadb-04
</span></span></code></pre></div><p>Now we are ready to detatch these volumes from VSCode and attach them to our target workload. I usually do this by rolling back the last change to VSCode:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>helm rollback code-server <span style="color:#ae81ff">0</span>
</span></span></code></pre></div><h3 id="do-the-switcheroo">Do the switcheroo</h3>
<p>The final step is now to swap the volumes from beneath our container. Most helm charts will have a <code>persistence</code> section in the <code>Values</code> file, where we can point it to our new volume using the <code>existingClaim</code> parameter. Here is an example from my mariadb helm chart (imported from Bitnami repo):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">mariadb</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">primary</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">persistence</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">existingClaim</span>: <span style="color:#ae81ff">gitea-mariadb-04</span>
</span></span></code></pre></div><p>Then we upgrade out deployment and fingers crossed we end up with a working deployment:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>helm upgrade gitea-mariadb .
</span></span></code></pre></div><p>One caveat here is that sometimes (and it depends on how the helm chart is structured) if you are using a <code>statefulSet</code> K8S deployment type, it will complain that such changes are not allowed on an <code>sts</code> and the deployment will fail. To remedy this you can modify the <code>sts</code> directly using kubectl edit command, replace just the volume name in its config, wait for the changes to take place, then run the <code>helm upgrade</code>. This should help you roll out this change without having to reinstalling the whole deployment.</p>
<h2 id="epilogue">Epilogue</h2>
<p>It definitely took me a few good sessions of manual tinkering to migrate all of my data to encrypted volumes. Some of them have been running for a few weeks now and I haven&rsquo;t had any problems with them so far. Evident lack of <code>dm-crypt</code> kernel module on ubuntu 22.04 was apparently also not an issue, the mounted volumes get decrypted and attached just fine. So was it all worth the efford? For the peace of mind, I would definitely say yes&hellip;</p>
]]></content>
        </item>
        
        <item>
            <title>Backup and Restore</title>
            <link>https://blog.octopusx.de/posts/backup_restore/</link>
            <pubDate>Fri, 04 Aug 2023 21:47:00 +0200</pubDate>
            
            <guid>https://blog.octopusx.de/posts/backup_restore/</guid>
            <description>So, you have a backup. Wait, two backups? And a third one off site too?? But&amp;hellip; You have tested the restore procedure, right? No!? Damn son, what do you need a backup for if you don&amp;rsquo;t even know if you can restore it???
If you can&amp;rsquo;t restore it, you don&amp;rsquo;t really have a backup&amp;hellip; Protecting your data doesn&amp;rsquo;t end at having a 3-2-1 backup. Whether you are self hosting services or handle company data in the cloud it pays to have a practiced plan for bringing your services and data back to life.</description>
            <content type="html"><![CDATA[<p>So, you have a backup. Wait, two backups? And a third one off site too?? But&hellip; You have tested the restore procedure, right? No!? Damn son, what do you need a backup for if you don&rsquo;t even know if you can restore it???</p>
<h2 id="if-you-cant-restore-it-you-dont-really-have-a-backup">If you can&rsquo;t restore it, you don&rsquo;t really have a backup&hellip;</h2>
<p>Protecting your data doesn&rsquo;t end at having a 3-2-1 backup. Whether you are self hosting services or handle company data in the cloud it pays to have a practiced plan for bringing your services and data back to life. If all you&rsquo;re going to do is read this section of the post and move on with your life, I&rsquo;d like you to take with you just this one key lesson: don&rsquo;t leave testing of your restore procedure until you actually need to restore! For anyone more technically minded I recommend to read through the log of a <a href="https://about.gitlab.com/blog/2017/02/01/gitlab-dot-com-database-incident/">GitLab outage from 2017</a>, where in the middle of an outage an employee accidentally deletes a live database, then realises that the S3 backups they thought they had were never actually working! Yikes! Let us see how I deal with backups and disaster recovery in my homelab using the insights I have learned working in the industry. All using open and free software!</p>
<h2 id="the-chicken-and-egg-situation">The chicken and egg situation</h2>
<p>So as we are talking about a homelab situation in this article, we are always going to be limited by certain inavitable factors. Like, most people have just one internet connection. Single source of electicity, a UPS in a best case scenario. What most homelabers also have in common is that they&rsquo;re building this entire stack in their own spare time. It often means that we can&rsquo;t just throw more hardware nor more elbow greese at the problems we encounter.</p>
<p>In a pevious article I outlined how I improved both my uptime and reliability by <a href="../homelab_ha_edition/">building a multinode k3s cluster</a>. In that post I have described a situation where I had to scrap my cluster almost entirely twice before I was able to get it just right. Something however that I did not do was have a decent way to protect my data while this all was happening, which meant that, at least for some applications. I had to rebuild my Nextcloud instance from scratch, but even worse, I lost my Gitea instance and all the automations I ran from it.</p>
<p>So, if the infrastructure needed to restore your infrastructure is gone, what can you do? Well, let&rsquo;s go over our options.</p>
<h2 id="how-do-i-back-up-so-far">How do I back up so far&hellip;</h2>
<p>Logically my storage solution has 3 tiers.</p>
<h3 id="tier-1">Tier 1</h3>
<p>Live data being accessed by my applications and services running in my k3s cluster.</p>
<p>This data is stored on fast NVME storage, and replicated across multiple physical Kubernetes nodes.
This tier of storage is completely managed by <a href="https://www.rancher.com/products/longhorn">Longhorn</a>.</p>
<h3 id="tier-2">Tier 2</h3>
<p>Backup of the live data hosted on the local network, using an NFS share on a virtualised <a href="https://www.truenas.com/truenas-scale/">Truenas Scale</a> node.</p>
<p>This data is stored on a redundant RaidZ2 array on a single node.
Data makes its way onto this Truenas instance using Longhorn&rsquo;s nighly snapshotting and backup mechanism.
Data on this tier is at most 24h old at any given time.</p>
<h3 id="tier-3">Tier 3</h3>
<p>Offsite copy of Tier 2 to another <a href="https://www.truenas.com/truenas-scale/">Truenas Scale</a> machine, using Truenas&rsquo; built in replication mechanism.</p>
<p>This data is also stored on a redundant RaidZ2 array on a single remote node.
This data makes its way onto the offsite Truenas instance via a Wireguard server hosted in the public cloud. Data is pushed in form of incremental backups and it is at most 48h old at any given time.</p>
<h3 id="visualisation">Visualisation</h3>
<p><img src="data_tiers.png" alt="Backup Flow Graph"></p>
<h2 id="longhorn-and-friends">Longhorn and friends</h2>
<p>So as you see this backup strategy relies heavily on Longhorn, which is a kubernetes storage orchestrator with powerful snapshotting and backup capabilities. What I found particularly interesting was that you can point multiple instances of Longhorn, each maintaining the storage of a completely separate Kubernetes cluster, to the same backup location. What this allows you to do is essentially use the backup and restore mechanism as a migration utility to move storage volumes between Kubrnetes clusters!</p>
<p>The beauty of this solution comes likewise from the fact that our workloads are running in Kubernets and that we are using Helm charts to deploy our applications. Helm charts which, conveniently, we are treating as our infrastructure as code.</p>
<p>Having both, the portability of application data (Longhorn-managed k8s storage volumes) and application configuration (helm charts) means that we have a repeatable, reliable way to migrate our workloads off site and spin them back up at will!</p>
<p>What is the last thing you need to get started? A good friend who will allow you to spin up an offsite k3s cluster on his Truenas box ;)</p>
<h2 id="the-nitty-gritty">The nitty gritty</h2>
<p>Let&rsquo;s go over the different moving parts that we need to set up to see how it all works.</p>
<h3 id="longhorn-helm-chart">Longhorn helm chart</h3>
<p>I am using the official longhorn helm chart:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># Chart.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v2</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">name</span>: <span style="color:#ae81ff">longhorn</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">version</span>: <span style="color:#ae81ff">0.0.1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">appVersion</span>: <span style="color:#ae81ff">v1.4.2</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">description</span>: <span style="color:#ae81ff">Longhorn is a distributed block storage system for Kubernetes.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">icon</span>: <span style="color:#ae81ff">https://raw.githubusercontent.com/cncf/artwork/master/projects/longhorn/icon/color/longhorn-icon-color.png</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">dependencies</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">longhorn</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">version</span>: <span style="color:#ae81ff">1.4.2</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">repository</span>: <span style="color:#ae81ff">https://charts.longhorn.io</span>
</span></span></code></pre></div><p>You can find all of the parameters in the github repo: <a href="https://github.com/longhorn/charts">https://github.com/longhorn/charts</a>, but what we want to focus here is the following configuration values:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># values.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">longhorn</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">defaultSettings</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">backupTarget</span>: <span style="color:#ae81ff">nfs://scale.octopusx.de:/mnt/hybrid/longhorn-backup</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">backupstorePollInterval</span>: <span style="color:#ae81ff">60</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">defaultLonghornStaticStorageClass</span>: <span style="color:#ae81ff">longhorn-static</span>
</span></span><span style="display:flex;"><span>    <span style="color:#ae81ff">(...)</span>
</span></span></code></pre></div><p>Notably, we need to set the <code>backupTarget</code> and the <code>defaultLonghornStaticStorageClass</code>.
The target will be an NFS share made available by the Truenas Scale running on my local network.
The static storage class will be a persistent volume class created inside of Kubernetes for all volumes that are created &ldquo;statically&rdquo;, i.e. not automatically as part of bootstrapping a new application. Using this method of provisioning the initial volume as well as restoring one from the backup means that the proces of bootstrapping this application is nearly identical for both the main instance and the backup one on the remote cluster.</p>
<h3 id="create-a-static-volume-for-gitea">Create a static volume for Gitea</h3>
<p>To create a new static volume for our Gitea to use, we go to the <code>Volumes</code> tab in Longhorn then click on <code>Create Volume</code> and fill in the form:
<img src="create_static_volume.png" alt="Create Static Volume"></p>
<p>Next click on the new volume and ask longhorn to create the PVC and PV for you inside Kubernetes:
<img src="create_pvc.png" alt="Create PVC"></p>
<h3 id="gitea-helm-chart">Gitea helm chart</h3>
<p>Normally you can provision new volumes dynamically, declaring the size and class. Since we are using a pre-provisioned volume and the PVC for it already exists, we can use the existing claim variable to pass it to our gitea chart.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">gitea</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">(...)</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">persistence</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">enabled</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">size</span>: <span style="color:#ae81ff">20Gi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#ae81ff">ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">existingClaim</span>: <span style="color:#ae81ff">data-gitea-02</span>
</span></span></code></pre></div><p>You&rsquo;ll obviously need the rest of the chart to deploy gitea, you can find <a href="https://dl.gitea.io/charts/">the template I use here</a>.</p>
<p>Deploying this chart should looks identically on my main Kubernetes cluster as well as the remote backup instance.</p>
<h3 id="longhorn-backup">Longhorn Backup</h3>
<p>If you are following along we should be able to access the Longhorn web UI, so long as you have enabled it in the helm chart:</p>
<p><img src="longhorn_ui.png" alt="Longhorn Web UI"></p>
<p>If all of this worked you should be able to go to the <code>Backup</code> tab and not see any connection errors. In my case, that NFS mount already contains earlier backups of various volumes I have backed up from a number of kubernetes clusters.</p>
<p><img src="longhorn_backup.png" alt="Longhorn Backups View"></p>
<p>Perfect! Now, we don&rsquo;t want to make backups manually. This is a homelab after all, it&rsquo;s is not our job to do all of the meneal tasks manually, it is to automate them away! The <code>Recurring Jobs</code> tab will let us do just that, here we can schedule nightly backups of whatever volumes we want. We can create a group label and label each volume with it to link it to a specific recurring task. For us, we will use the default group, which is already attatched to every single volume, like so:</p>
<p><img src="nightly_backup.png" alt="Longhorn Recurring Job Form"></p>
<p>And that is it for the Longhorn backups, this will trigger a kubernetes job every night at the selected time and do the work. The backups are incremental so it is storage efficient if we desire to keep multiple backups for each volume by setting the <code>Retain</code> value to anything above 1. Once a backup is performed you can verify it by going to the <code>Backup</code> tab or any volume specifically, like this gitea data volume that we use for this example:</p>
<p><img src="gitea_volume.png" alt="Gitea Volume"></p>
<h3 id="offsite-backup">Offsite Backup</h3>
<p>With backups of kubernetes cluster PVCs securely pushed every night to our Truenas server via NFS, the next step for me was to do the push from tier 2 (local backup) to the 3rd tier (offsite backup). This I achieve via the use of Truenas replication feature. I won&rsquo;t be covering this step in detail as the replication itself is a well documented feature, so I will send anyone interested to the <a href="https://www.truenas.com/docs/core/coretutorials/tasks/creatingreplicationtasks/remotereplication/">official Truenas documentation</a>.</p>
<p>As for how do my local Truenas host talks to the remote Truenas, I use Wireguard VPN. Technically Truenas scale doesn&rsquo;t have the option to configure Wireguard in the UI (though OpenVPN is an option), but Wireguard binaries are preinstalled (at least they are at the time of writing, I am currently running TrueNAS-22.12.3.2 Bluefin). This means that you can easily <a href="(../wireguard/)">follow my previous post</a> and simply add both the local and remote Truenas hosts to your Wireguard network via your cloud-hosted Wireguard server.</p>
<h3 id="restore-from-offsite-backup">Restore from offsite backup</h3>
<p><img src="restore.drawio.png" alt="Restore Flow Graph"></p>
<p>Restoring from backups is very simple in Longhorn. On our remote backup K8S instance we will go to the Longhorn Web UI, the <code>Backup</code> tab, then select the volume to restore:
<img src="restore_01.png" alt="Select restoration target"></p>
<p>Create the volume with the same name as the previously backed up volume:
<img src="restore_02.png" alt="Configure restored volume"></p>
<p>Ask longhorn to create the PVC and PV for you inside Kubernetes:
<img src="create_pvc.png" alt="Create PVC"></p>
<p>The rest of the steps is identical to bootstrapping the original Gitea instance! Just make it use the existing, restored PVC which conveniently already has the correct name :)</p>
<h3 id="switching-traffic-to-the-offsite-backup">Switching traffic to the offsite backup</h3>
<p>As it can be seen on the beautiful visualisation I produced, the way I access services in my homelab from the outside world is by hosting a HA proxy in the public cloud. A subnet router node is deployed on both my Homelab LAN and Offsite LAN (not actually represented on the graph) that is able to route traffic to the respective networks. This means that switching traffic from one deployment to the other is as simple as changing a few lines of code in the HAProxy config:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#a6e22e">frontend traefik_public</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a6e22e">bind *:443</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a6e22e">default_backend    homelab # &lt;- We change homelab to offsite here</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a6e22e">option             forwardfor</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Homelab LAN Backend</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">backend homelab</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Use roundrobin to balance traffic</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a6e22e">balance            roundrobin</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Define the backend servers</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a6e22e">server             backend01 192.168.50.50:443 check</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Offside LAN Backend</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">backend offsite</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Use roundrobin to balance traffic</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a6e22e">balance            roundrobin</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Define the backend servers</span>
</span></span><span style="display:flex;"><span>        <span style="color:#a6e22e">server             backend01 192.168.150.50:443 check</span>
</span></span></code></pre></div><p>Then we restart the HAProxy service, all done!</p>
<h2 id="the-neat-outcome-of-it-all">The neat outcome of it all</h2>
<p>So, for those of you that are more familiar with Kubernetes, you will have spotted that I omitted a number of other things that have to be synced between the two clusters in order to be able to switch traffic freely between them. For a start, Gitea has a database, so you will need to backup its volume and restore that too. However, the neat thing about this solution is, the backup and restore procedures are identical for each and every volume, and they should work just as well, so I didn&rsquo;t feel it was worth doubling up on the explanation.</p>
<p>Another, and crucially, untouched topic here would be certificates. One way to solve this cleanly would be to resolve TLS at the HA proxy, which means you don&rsquo;t need to do anything with the certs when you fail over. Alternatively you would have to sync your certificates between the clusters in some other way, or use features of Traefik or another ingress controller to generate all of the certs you want on both systems in parallel.</p>
<p>Lastly, how I actually use this in practice, and how I recommend you do it - the failover is fancy and shows the power of this solution, but I just use the wireguard network I already have to access my backups if necessary. More importantly, thanks to this wireguard network I can perform these restores once a month or so, to make sure the whole process still works. Because, as I alluded to at the very beginning, if you haven&rsquo;t tested your restore procedure, do you even really have a backup?</p>
]]></content>
        </item>
        
        <item>
            <title>HA in the homelab</title>
            <link>https://blog.octopusx.de/posts/homelab_ha_edition/</link>
            <pubDate>Thu, 08 Jun 2023 14:46:20 +0200</pubDate>
            
            <guid>https://blog.octopusx.de/posts/homelab_ha_edition/</guid>
            <description>In general I like things simple. I need technology to work for me and provide value without having to dedicate extraordinary amount of time to it. When it comes to running a homelab this is especially true, since I am likely spending my own free time setting things up and maintaining my infrastrcuture. At a certain point in the life of my homelab however I found myself hosting services for my wife, family and some friends.</description>
            <content type="html"><![CDATA[<p>In general I like things simple. I need technology to work for me and provide value without having to dedicate extraordinary amount of time to it. When it comes to running a homelab this is especially true, since I am likely spending my own free time setting things up and maintaining my infrastrcuture. At a certain point in the life of my homelab however I found myself hosting services for my wife, family and some friends. This meant that I started considering a new challenge, the challenge of keeping my services up so that the people that rely on them do not face unpleasant disruptions.</p>
<h2 id="the-humble-beginning">The Humble Beginning</h2>
<p>In the beginning I hosted everything on a lone Unraid server, which I built by converting an old desktop machine I had laying around, a very common beginning for a homelab. Unraid gives you a very economical way to manage mass storage with some redundancy, with the ability to run docker containers and mount their volumes to either the spinning rust array, or an ssd cache drive.</p>
<p>This has worked ok for a few years, but in time 3 problems started grating on me:</p>
<ul>
<li>If you want to do any maintenance of the server, you must spin the disk array down. And by maintenance I meana changing most basic settings, such as network or storage configuration. And every time you stop the array, you must shut down the VM and docker engines for the duration.</li>
<li>There were multiple times where failure to resolve dns meant that my containers would fail to update, then promptly &ldquo;disappear&rdquo;, together with all their configuration. These were configured manually and now had to be rebuilt manually&hellip;</li>
<li>Every time one of these events occured, all of my homelab users would be inconvenienced. Long periods of unavailability is unacceptable if you want to build a system that multiple people rely on on daily bases.</li>
</ul>
<h2 id="make-it-double">Make it double</h2>
<p>I decided to experiment with a HA setup and build a second unraid server. This involved abandoning Unraid&rsquo;s built in docker engine (for the most part), and choosing an alternative way to orchestrate my workloads.
My first choice was Microk8s from Cannonical, as it was the simplest way to create a HA multinode cluster.</p>
<p>The goal of this setup was to allow me to take one of the nodes down for &ldquo;maintenance&rdquo; and have the services remain available, running on the second node.</p>
<p>The initial idea for how to design this system was basically a mirror setup. Each of the Unraid nodes would have:</p>
<ul>
<li>2 large HDDs for the main Unraid array, storing the data</li>
<li>1 NVME SSD used as a separate &ldquo;cache pool&rdquo; for storing the Unraid VM and Docker engine data</li>
<li>A docker container running Pihole</li>
<li>A number of VMs running a Kubernetes cluster</li>
</ul>
<h3 id="what-are-we-running">What are we running</h3>
<p>The most important application that we want to host is Nextcloud, with the intention of using it as photo backup for all family phones. To achieve our functionality we needed to run at minimum the following applications in our kubernetes cluster:</p>
<ul>
<li>Metallb</li>
<li>Traefik</li>
<li>Nextcloud Server</li>
<li>Redis</li>
<li>Mariadb</li>
</ul>
<h3 id="storage">Storage</h3>
<p>3 out of the 5 services needed are actually stateful applications with requirements to have some form of either object or block storage. The most obvious solution that comes to mind is to leverage the NFS server hosted on our 2 Unraid servers. It was not a viable option because it would again create a single point of failure. You can not run pure NFS with any sort of failover or HA, which means that it doesn&rsquo;t matter that you have 2 unraid servers, the moment you take the wrong one down for &ldquo;maintenance&rdquo; the whole system collapses.</p>
<p>I started researching different different distributed filesystem solutions that can be used to provision storage for kubernetes nodes, and I came across the following:</p>
<ul>
<li>CephFS + Rook</li>
<li>GlusterFS</li>
<li>Longhorn</li>
</ul>
<p>I have looked into Ceph for a little while before deciding it is too complex for my simple setup.
I tried to setup GlusterFS but failed to make it work. I also found information that the GlusterFS Kubernetes provisioner was getting deprecated, which in the end led me to just one realistic choice, Longhorn.
I looked at Longhorn last because I was hasitant to use a non-generic storage solution from Rancher. I generally try to stick to CNCF and proven open-source projects in my homelab, as I did not want to invest time into setting up infrastructure that may get &ldquo;deprecated&rdquo; or &ldquo;abandoned&rdquo; and have to re-build everything.
However, I did manage to make longhorn work on my MicroK8S cluster, and I was off to the races with my new HA setup.</p>
<p>The Longhorn documentation mentions that for best results they recommend a high IOPS NVME drive for storage, so I added a couple of NVME SSDs into each of my Unraid servers to pass through to the VMs for that purpose.</p>
<h3 id="first-attempt">First attempt</h3>
<p>For simplicity&rsquo;s sake, I decided I not to create an overly complex and maintenance-heavy system and stayed away from fully featured enterprise-grade K8S solutions. Having played with MicroK8S from Cannonical for a while in a local VM I decided to give it a test run in cluster mode.
I have initially created a very simple 2 virtual machine setup, one machine on each Unraid server.
I passed each its own NVME disk and exposed it to Longhorn and everything was fine&hellip; For a while&hellip; Until it wasn&rsquo;t fine anymore.</p>
<p>The first time I went to reboot one of the Unraid servers the MicroK8S cluster desintegrated.
In order to maintain operation of the system I had set Longhorn to have 2 replicas of each persistent volume, so one on each node. This means that when one of the nodes comes back after being unavailable for a while, it will discard its copy of the volume and sync it from the nodes that remained available in the cluster. This syncing operation is very IOPS and CPU heavy, to the point that while the sync operation was taking place, the K8S control plane (ETCD) was starved for cycles and the cluster had gone out of sync entirely and failed to come back.</p>
<p>Some of you might say, well, you should have known this right? This is why you shouldn&rsquo;t host the control plane and data plane on the same host! It is asking for trouble&hellip; Ok, what should we do about it then? To save some words, I tried creating separate MicroK8S nodes for the control plane, data plane and storage, so that I woud have one of each type of nodes, each on a separate VM across the 2 Unraid boxes, and TLDR, it doesn&rsquo;t work because MicroK8S doesn&rsquo;t let you choose which nodes are supposed to act as masters and which are to be workers. The first 3 nodes to be added to the cluster would form the control plane, as necessary to reach quorum in ETCD. Each next added node would not be a part of the control plane. The moment you reboot one of the &ldquo;master&rdquo; nodes though (as long as another node in the cluster is available) another one will pick up that role and you have no say in which one that would be. After a few weeks the cluster has died in much the same way as it did initially. We needed a different solution&hellip;</p>
<h3 id="move-to-k3s">Move to K3S</h3>
<p>It seems to be a bit of a theme at this point, but Rancher Labs seems to know what they&rsquo;re doing. Aside from MicroK8S, their K3S distribution of Kubernetes is another viable and likely the most popular option for Kubernetes on the edge and in the homelab. It is lightweight and flexible, allowing you to dedicate nodes specifically to the control plane and even using alternative data stores for control plane state.</p>
<h3 id="final-touches">Final touches</h3>
<p>Just like before, we have built a 6 VM Kubernetes cluster, 2 master nodes, 2 workers and 2 longhorn storage nodes. To keep the system as vanilla as possible I elected for the default storage technology, which is ETCD. Such a system however is not HA, to have quorum you need 3 ETCD hosts. To ensure that the cluster stayed up while one of the Unraid servers is taken down I decided to host the third ETCD server&hellip; on a spare RaspberryPi I had laying around of course!</p>
]]></content>
        </item>
        
        <item>
            <title>Wireguard &#43; Linode &#43; Unraid = Profit</title>
            <link>https://blog.octopusx.de/posts/wireguard/</link>
            <pubDate>Sat, 27 Aug 2022 15:24:20 +0200</pubDate>
            
            <guid>https://blog.octopusx.de/posts/wireguard/</guid>
            <description>I have been using Unraid as a platform to manage my storage at home as well as hosting a few basic services for me and my family for a good few years now. Despite having a fairly fast internet connection I was not able to expose those services to the internet however as my home router does not support dynamic DNS. As an alternative I knew that I could get a small VM with a public IP address which could host a VPN endpoint for me to build a permanent tunnel to my Unraid box.</description>
            <content type="html"><![CDATA[<p>I have been using Unraid as a platform to manage my storage at home as well as hosting a few basic services for me and my family for a good few years now.
Despite having a fairly fast internet connection I was not able to expose those services to the internet however as my home router does not support dynamic DNS.
As an alternative I knew that I could get a small VM with a public IP address which could host a VPN endpoint for me to build a permanent tunnel to my Unraid box.
For a long time I avoided this as setting up your own VPN seemed a daunting and needlessly complicated task. Then I&rsquo;ve learned of Wireguard.
It lets you set up temporary and permanent tunnels with ease, it is simple to manage and supposed to provide state-of-the art security.
Join me on a short trip to find out how you can set up your own easy-to-manage VPN and access your home network resources from anywhere.</p>
<h2 id="the-public-endpoint">The public endpoint</h2>
<p>To be able to host a VPN you can connect to from anywhere you will need a machine with a public IP.
As mentioned before, this can be solved by setting up DDNS instead, but if you&rsquo;re like me and you don&rsquo;t have that option this is a solid alternative.
The easiest way to obtain a public IP address is to create a small cheap VM with a cloud provider to host your Wireguard VPN.
In my setup I have used Linode, but you can use anything you like really, so long as it gives you that sweet public IP goodness.</p>
<h3 id="provision-the-host">Provision the host</h3>
<p>We are going to build our Linode wireguard VPN on an Ubuntu linux VM, version 20.04.
Our VM should have wireguard available as a package out of the box which make installation super easy:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt update <span style="color:#f92672">&amp;&amp;</span> apt install -y wireguard resolvconf ufw
</span></span></code></pre></div><p>It is probably also a good idea to enable firewall on this host. For now, we can safely block all traffic except ssh and wireguard ports:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ufw allow ssh
</span></span><span style="display:flex;"><span>ufw allow 51820/udp
</span></span><span style="display:flex;"><span>ufw enable
</span></span></code></pre></div><p>If you&rsquo;ve run the above and are still able to connect to your server over ssh, you&rsquo;ve done well. If your server is now turning a blind eye to your ssh connection attempts, you may have to access it over linode shell, then double check you&rsquo;ve blocked the correct ports and fix the firewall rules if necessary.</p>
<h3 id="configure-wireguard">Configure Wireguard</h3>
<p>Once we got the software cosily installed we have to configure the bad boy. We can start with generating a keypair for the server.
To do that run the following command on your cloud instance:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wg genkey | tee private.key | wg pubkey &gt; public.key
</span></span></code></pre></div><p>This will create the two key files for you, private and public. The contents of the private key will live in the config on the server while the public one has to be shared with every peer that wishes to connect to this server. Either way, it is a good idea to think about where you&rsquo;re gonna store these keys for safekeeping. I recommend using a secure note in your password manager app or an encrypted backup drive. You&rsquo;ve got one of these already, right? Right!?</p>
<p>The next step is to create the server configuration for your public endpoint.
The location of all of the endpoints will be stored in <code>/etc/wireguard</code>. We are going to call ours <code>wg0.conf</code>. You will notice that once we are done there will be a new virtual network interface on your linux host also called <code>wg0</code>. That&rsquo;s because wireguard will create a virtual interface per tunnel endpoint that you configure, but more on that later ;)</p>
<p>Take a look at the following example config file:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#66d9ef">[Interface]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Address</span>    <span style="color:#f92672">=</span> <span style="color:#e6db74">10.0.0.1/24</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">DNS</span>        <span style="color:#f92672">=</span> <span style="color:#e6db74">192.168.1.10,my-search-domain.com</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">SaveConfig</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">false</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">ListenPort</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">51820</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">PrivateKey</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">mysupersecretgeneratedprivatekey</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">PostUp</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">PostDown</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">[Peer]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">PublicKey</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">mylesssecretbutalsosecretpeerpublickey</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">AllowedIPs</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">10.0.0.2/32, 192.168.1.0/24 # The networks that traffic can be routed to over this peer connection</span>
</span></span></code></pre></div><p>The above block is a good starting point, let&rsquo;s go over its contents briefly.</p>
<p>The <strong>interface</strong> block is the configuration parameters for our <code>wg0</code> virtual network interface. It also configures some server-side parameters, such as what is the address of the DNS server to be used to resolve domain names on our VPN network. This is useful if you are hosting a DNS server within your lan to resolve internal domain names.</p>
<p>Another field we can see is the port, which is the network port to be used by peers to establish a connection with the server. For safety reasons (security by obscurity) it is a nice idea to change it to anything else other than the default.</p>
<p>We will also need to whitelist this address on our firewall and any security group analogs your cloud provider offers. Lastly we have the private key where, yes, you guessed it, we paste the private key that you so diligently saved in your password manager, haven&rsquo;t you?</p>
<p><code>PostUp</code> and <code>PostDown</code> fields allow us to run arbitrary code triggered by the change of our WireGuard tunnel state. Names are quite self-explanatory. Remember, just because we are able to create the tunnel, doesn&rsquo;t mean we will send any traffic through it. It is by modifying network policies after the tunnel is established (and subsequently tearing them down when we take the tunnel down) that we are able to funnel our traffic where it is meant to go.</p>
<p>The second block we can see is the <strong>peer</strong> block.</p>
<p>The first field would be a public key. And no, this is not our server&rsquo;s public key, this is the public key of your peer. A peer in this case is any machine trying to connect to our vpn server. Therefore you will likely have many <strong>peer</strong> blocks in this secrion, as many as there are devices you want to allow to connect to your network.</p>
<p>The allopwed IPs field will tell the server what range of IP addresses (subnets) it will accept traffic from should the handshake with the matching peer be successful. This will be the IP address of the <code>wg0</code> (or whatever you call name it) interface BUT ON THE PEER. Plus any other network this other peer will expose to us.</p>
<p>If this is a little confusing, don&rsquo;t worry, we&rsquo;ll be able to connect the dots a little later.</p>
<h2 id="reaching-the-unraid-server">Reaching the Unraid server</h2>
<p>Unraid is a very potent platform for managing your home server infrastructure and it makes it extremely easy to deploy services within your home network.</p>
<p>I myself host a Pihole and HomeAssistant containers on mine among others.
It also has the ability to install extensions. When trying to figure out an easy and reliable way to set up a permanent tunnel to my Linode wireguard host I stumbled accross a <a href="https://forums.unraid.net/topic/84226-wireguard-quickstart/">nicely written extension</a> which at first seemed like it would do the trick.</p>
<p>After some attempts to make it work however I realised that it won&rsquo;t do the trick, I could not get my Unraid box to forward traffic from the VPN to the rest of my network, which meant my internally-hosted services would not be reachable by other devices on the VPN. Mind you this was the beginning of my journey and I was only learning about wireguard so quite possibly it&rsquo;s entirely  my fault. If you have managed to make it work, more power to you!</p>
<p>In the end I decided to go for a small VM instead. Seeing as setting up wireguard on an Ubuntu machine is so quick and simple this went very smoothly.</p>
<h3 id="provision-the-vm">Provision the VM</h3>
<p>First, use the Unraid UI to spawn a new Ubuntu Server 20.04 VM. I assigned mine 2 vCPUs and 2GB of RAM, which should be plenty enough for the job. You can adjust this later anyway if you decide it is under or over-provisionned, so no biggie&hellip;</p>
<p>Once you&rsquo;ve got it up and running, install the wireguard package just like on the Linode:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt update <span style="color:#f92672">&amp;&amp;</span> apt install -y wireguard resolvconf
</span></span></code></pre></div><p>Just like before, we will need some freshly baked key material, straight from the oven:
To do that run the following command on your cloud instance:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wg genkey | tee private.key | wg pubkey &gt; public.key
</span></span></code></pre></div><p>I called my VM and configured it to have the hostname of <code>wireguard-gateway</code>, so we will refer to it as such from now on.
Yup, nothing more to see here, let&rsquo;s move on&hellip;</p>
<h3 id="configure-a-permanent-tunnel">Configure a permanent tunnel</h3>
<p>On the <code>wireguard-gateway</code> VM we will need to setup the basic wireguard tunnel peer config with a few small extras.
Let&rsquo;s take a look at what my config includes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#f92672">[</span>Interface<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>PrivateKey <span style="color:#f92672">=</span> privatekeyofthewireguard-gateway
</span></span><span style="display:flex;"><span>Address <span style="color:#f92672">=</span> 10.0.0.3/24
</span></span><span style="display:flex;"><span>PostUp <span style="color:#f92672">=</span> iptables -A FORWARD -i wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE; ip6tables -A FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
</span></span><span style="display:flex;"><span>PostDown <span style="color:#f92672">=</span> iptables -D FORWARD -i wg0 -j ACCEPT; iptables -t nat -D POSTROUTING -o eth0 -j MASQUERADE; ip6tables -D FORWARD -i wg0 -j ACCEPT; ip6tables -t nat -D POSTROUTING -o eth0 -j MASQUERADE
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>Peer<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>PublicKey <span style="color:#f92672">=</span> publickeyofthewireguardlinodehost
</span></span><span style="display:flex;"><span>Endpoint <span style="color:#f92672">=</span> my-linode-wireguard.com:51820
</span></span><span style="display:flex;"><span>AllowedIPs <span style="color:#f92672">=</span> 0.0.0.0/0
</span></span><span style="display:flex;"><span>PersistentKeepalive <span style="color:#f92672">=</span> <span style="color:#ae81ff">20</span>
</span></span></code></pre></div><p>Well now that finally looks like some black magic. If you&rsquo;re familiar with <code>iptables</code> it likely makes perfect sense, I had to learn through strategic use of duckduckgo and trial-and-error&hellip;</p>
<p>The important bits here are the <code>PostUP</code> and <code>PostDown</code> commands, which are hooks to be executed when the tunnel is set to be up or down respectively. Here we are:</p>
<ul>
<li>Enabling forwarding of any traffic that enters through the wireguard interface <code>wg0</code></li>
<li>Enabling NAT Masquarading when traffic is forwarded to <code>eth0</code> to allow it to find its way back to the VPN subnet
Then the <code>PostDown</code> reverts the changes, to make sure routing works correctly also when the tunnel is down.</li>
</ul>
<p>Since this machine is going to be the one establishing the connection with the Linode wireguard host as it itself lives behind the nasty nasty NAT, we want to set the <code>PersistentKeepalive = 20</code> here too. Wireguard&rsquo;s noise protocol will not transfer any data whatsoever unless actual data is requested or sent by peers on either side of the connection. This means that your host behind the layer of NAT will become unavailable after a little while unless you artificially keep pinging it via this setting, extending the lifespan of this connection.</p>
<ul>
<li>Enable forwarding</li>
<li>Use systemd to start and enable the wg service, make sure it auto-reconnects on reboot</li>
</ul>
<h3 id="launch-the-guard-of-wires">Launch the guard of wires</h3>
<p>There are a couple of more steps necessary to turn our <code>wireguard-gateway</code> into a defacto gateway router. First of all, generic linux will have packet forwarding disabled by default. We will need to enable it and permanently, so that the setting survives any planned (or indeed unplanned) reboots and shutdown.</p>
<p>Go and add the following line to <code>/etc/sysctl.conf</code>:</p>
<pre tabindex="0"><code>net.ipv4.ip_forward=1
</code></pre><p>To check that it worked run the following command:</p>
<pre tabindex="0"><code>sysctl net.ipv4.ip_forward
</code></pre><p>The expected return value if it worked is this:</p>
<pre tabindex="0"><code>net.ipv4.ip_forward = 1
</code></pre><p>We should be able to to turn on our wireguard tunnel at this point. And for that, we have a special magic command:</p>
<pre tabindex="0"><code>sudo systemctl start wg-quick@wg0
</code></pre><p>Why this magic command? <code>wg-quick</code> is a utility that comes with wireguard and it lets you manage wg tunnels easliy as a human being and not a soulless robot. The above will not only start the tunnel but also enable a systemd service for it, meaning that the tunnel will remain up even after reboots. Nice.</p>
<h3 id="quick-reality-check">Quick reality check</h3>
<p>Here if everything went according to plan we should be able to start testing this new tunnel. The first thing you&rsquo;ll likely want to do is make sure traffic is routed within the wireguard network itself.</p>
<p>Let&rsquo;s (IP) address all of the hosts from the start.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># wireguard server</span>
</span></span><span style="display:flex;"><span>wg0 <span style="color:#f92672">=</span> 10.0.0.1
</span></span><span style="display:flex;"><span>eth0 <span style="color:#f92672">=</span> 171.172.173.174
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># unraid vm</span>
</span></span><span style="display:flex;"><span>wg0 <span style="color:#f92672">=</span> 10.0.0.2
</span></span><span style="display:flex;"><span>eth0 <span style="color:#f92672">=</span> 192.168.1.5
</span></span></code></pre></div><p>Let&rsquo;s log into the Wireguard server and ping some stuff:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>~ ‚ùØ ping 10.0.0.5 -c <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>PING 10.0.0.5 <span style="color:#f92672">(</span>10.0.0.5<span style="color:#f92672">)</span> 56<span style="color:#f92672">(</span>84<span style="color:#f92672">)</span> bytes of data.
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">64</span> bytes from 10.0.0.5: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span> time<span style="color:#f92672">=</span>29.6 ms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>--- 10.0.0.5 ping statistics ---
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">1</span> packets transmitted, <span style="color:#ae81ff">1</span> received, 0% packet loss, time 0ms
</span></span><span style="display:flex;"><span>rtt min/avg/max/mdev <span style="color:#f92672">=</span> 29.618/29.618/29.618/0.000 ms
</span></span></code></pre></div><p>This means we can communicate with our Unraid VM. Sweet.</p>
<p>The bigger question is, can we also access resources on our LAN, behind our Wireguard gateway? Well, we can test that too.
Find a local IP address on your home network that will respond to ping and use that. We are going to assume we have a Pihole server running under <code>192.168.1.10</code>.</p>
<pre tabindex="0"><code>~ ‚ùØ ping 192.168.1.10 -c 1
PING 192.168.1.10 (192.168.1.10) 56(84) bytes of data.
64 bytes from 192.168.1.10: icmp_seq=1 ttl=63 time=20.9 ms

--- 192.168.1.10 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 20.922/20.922/20.922/0.000 ms
</code></pre><p>Now, pinging traffic is special, in fact it is so special it uses its own specific protocol called <code>icmp</code>. What I found while working on this setup is, just because <code>icmp</code> traffic makes it through, it doesn&rsquo;t mean that you&rsquo;re all good just yet. Since we are using a <code>Pihole</code> endpoint to test the ping, and we know <code>Pihole</code> has a web UI, let&rsquo;s try to curl that too.</p>
<pre tabindex="0"><code>~ ‚ùØ curl -I http://192.168.1.10/admin/
HTTP/1.1 200 OK
Set-Cookie: PHPSESSID=4215ktt1igaqf8qoj5llbhv376; path=/; HttpOnly
Expires: Thu, 19 Nov 1981 08:52:00 GMT
Cache-Control: no-store, no-cache, must-revalidate
Pragma: no-cache
Content-type: text/html; charset=UTF-8
X-Pi-hole: The Pi-hole Web interface is working!
X-Frame-Options: DENY
Date: Sat, 27 Aug 2022 13:16:15 GMT
Server: lighttpd/1.4.59
</code></pre><p>See? Now we are pretty sure it works. At least it will do for now and we can perform more practical tests once we set up wireguard on our end client machine.</p>
<h2 id="the-rest">The rest</h2>
<h3 id="client-config-on-ubuntu">Client config on Ubuntu</h3>
<p>Now, this one gave me a lot of trouble for a while but I am very happy that I managed to figure it out all on my own, like a propper big boy with access to stack overflow! Let&rsquo;s take a look at my final configuration file first, as I assume that&rsquo;s mainly what you&rsquo;re here for and postpone story time for a moment.</p>
<pre tabindex="0"><code>[Interface]
Address = 10.0.0.20/24
DNS = 192.168.1.10,my-search-domain.com
PrivateKey = myclientprivatekeypleasedontshare
MTU = 1280

[Peer]
AllowedIPs = 0.0.0.0/0
Endpoint = my-linode-wireguard.com:51820
PublicKey = thepublickeyofmylinodevpswireguarserverendpoint
</code></pre><p>This configuration block lives on my laptop in the usual place, i.e. <code>/etc/wireguard/wg0.conf</code>.</p>
<p>We will start from the <strong>peer</strong> block for a change.
Allowed IPs here should be set to <code>0.0.0.0/0</code>, which means that all of the traffic generated by our host will go through the tunnel. We can use this setting to adjust what goes through the tunnel and if, for example, we limit the subnet to <code>10.0.0.0/24,192.168.1.0/24</code>, then the VPN subnet and our homelab subnet will be routed through the VPN tunnel, while all other internet traffic will be ommited and shot out straight onto the interwebz.</p>
<p>Endpoint will be identical for the wireguard-gateway and all of the peers that are not the VPS machine itself. It is the public address of the VPS machine, followed by the VPS&rsquo; public key to allow this peer to authenticate.</p>
<p>The <strong>interface</strong> configuration, as always, refers to the virtual wg0 interface on my laptop. The address will be the IP my client wants to assume within the VPN subnet.</p>
<p>The DNS entry just as before is telling my computer where to reach my internal domain name server and what my private search domain is.</p>
<p>The private key is to be generated and stored safely, as all of the previously generated keys.</p>
<h3 id="story-time">Story time</h3>
<p>Last but not least, the <code>MTU</code>. This is an optional parameter and was at the center of my problems with this entire setup.
Generally most people do not need to touch this, though hopfully this will be a cautionary tale for anyone experiencing weird issues with their wireguard tunnels.
My first attempt of making my roadwarrior peer working omitted this parameter, as most guides and vanilla setup documentations do not mention needing it. Indeed my VPN tunnel would connect without it. I could ping all of the hosts on the VPN subnet as well as my private subnet behind NAT. I could even resolve DNS. But when I tried to perform any other type of data transfer over the tunnel however, it would fail.</p>
<p>To troubleshoot this I had tested reachability from and to each host on my network, and all would work besides connections to and from my roadwarrior peer.
This would have indicated that either my <code>wireguarad-gateway</code> host is not forwarding traffic correctly to the VPN subnet, or that there is an error in the VPN tunnel configuration on the VPS. At the time however I already had one working android peer, which could access and use all of the services hosted in my private subnet via the wireguard tunnel, so neither of the aformentnioned was a likely conclussion.</p>
<p>I have resorted to take an even closer look at what is happening on the network when I try to establish an ssh connectino between my roadwarrior peer and an ssh server in my private subnet. I set up probes listenning for ssh traffic on all 4 contact points:</p>
<ul>
<li>the roadwarrior peer</li>
<li>the vps server</li>
<li>the wireguard-gateway</li>
<li>the target server in the private subnet</li>
</ul>
<p>What I observed was that connection request would make all of the hops starting from the roadwarrior and ending on the targert server. Return traffic originating from the target server however could be observed on the target server itself, the wireguard-gateway, then the vps server, but the trail would end there. This was an indicator that it is strictly a problem with my ubuntu roadwarriror host.</p>
<p>After some frantic duckduckgoing and many a-stackoverflow page overturned, I found our that my probems are sympthomatic for MTU-related issues. This means that my laptop is not accepting packets exceeding certain size, and therefore dropping most of the traffic returning to it from my wireguard tunnel. Setting a smaller-than-default MTU made it work instantly, and every tunnel and virtual interface lived happy ever after&hellip;</p>
<h2 id="epilogue">Epilogue</h2>
<p>So since homelabs are pretty much living beings, by the time I finished writing this blog entry my Wireguard setup has evolved and is used for much more than what is described here. What I must say though, I have myself refered back to this guide when trying to figure out issues with my own setup as well as when helping friends erect their own self hosted VPNs, so I am quite happy with the contents here and hope someone will benefit from reading this document. As to the parts that I omitted in this entry, I guess you will have to check back in to find out the missing bits and peaces as I scatter them across the field of future blog entries ;)</p>
]]></content>
        </item>
        
    </channel>
</rss>
