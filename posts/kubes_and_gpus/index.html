<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="">
<meta name="description" content="Graphics cards are once again available. The AI hype is on for sure, but it&amp;rsquo;s not as much of a craze as the the mining fad of yesteryear. Finally, graphics-based compute is semi-affordable again, especially if you look to the second hand market. Having dabbed my toes in the world of self hosting large language models I have come to realize that hosting those on CPUs only is not going to give me a good insight into what this tech is capable of." />
<meta name="keywords" content="" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://blog.octopusx.de/posts/kubes_and_gpus/" />


    <title>
        
            Kubes and GPUs :: Developer and Self Hoster Blog  — A simple theme for Hugo
        
    </title>





<link rel="stylesheet" href="../../main.b78c3be9451dc4ca61ca377f3dc2cf2e6345a44c2bae46216a322ef366daa399.css" integrity="sha256-t4w76UUdxMphyjd/PcLPLmNFpEwrrkYhajIu82bao5k=">



    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../site.webmanifest">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="">
    <link rel="shortcut icon" href="../../favicon.ico">
    <meta name="msapplication-TileColor" content="">


<meta itemprop="name" content="Kubes and GPUs">
<meta itemprop="description" content="Graphics cards are once again available. The AI hype is on for sure, but it&rsquo;s not as much of a craze as the the mining fad of yesteryear. Finally, graphics-based compute is semi-affordable again, especially if you look to the second hand market. Having dabbed my toes in the world of self hosting large language models I have come to realize that hosting those on CPUs only is not going to give me a good insight into what this tech is capable of."><meta itemprop="datePublished" content="2023-10-30T13:00:00+02:00" />
<meta itemprop="dateModified" content="2023-10-30T13:00:00+02:00" />
<meta itemprop="wordCount" content="2451"><meta itemprop="image" content="https://blog.octopusx.de/"/>
<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.octopusx.de/"/>

<meta name="twitter:title" content="Kubes and GPUs"/>
<meta name="twitter:description" content="Graphics cards are once again available. The AI hype is on for sure, but it&rsquo;s not as much of a craze as the the mining fad of yesteryear. Finally, graphics-based compute is semi-affordable again, especially if you look to the second hand market. Having dabbed my toes in the world of self hosting large language models I have come to realize that hosting those on CPUs only is not going to give me a good insight into what this tech is capable of."/>



    <meta property="og:title" content="Kubes and GPUs" />
<meta property="og:description" content="Graphics cards are once again available. The AI hype is on for sure, but it&rsquo;s not as much of a craze as the the mining fad of yesteryear. Finally, graphics-based compute is semi-affordable again, especially if you look to the second hand market. Having dabbed my toes in the world of self hosting large language models I have come to realize that hosting those on CPUs only is not going to give me a good insight into what this tech is capable of." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.octopusx.de/posts/kubes_and_gpus/" /><meta property="og:image" content="https://blog.octopusx.de/"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-10-30T13:00:00+02:00" />
<meta property="article:modified_time" content="2023-10-30T13:00:00+02:00" /><meta property="og:site_name" content="Developer and Self Hoster Blog" />







    <meta property="article:published_time" content="2023-10-30 13:00:00 &#43;0200 &#43;0200" />











    </head>

    
        <body>
    
    
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text ">
                kubectl get pods</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="../../posts">blog</a></li><li><a href="../../feed">feed</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            
        </span>
    </span>
</header>


            <div class="content">
                
  <main class="post">

    <div class="post-info">
      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock">
          <circle cx="12" cy="12" r="10"></circle>
          <polyline points="12 6 12 12 16 14"></polyline>
        </svg>
        12 minutes

        
      </p>
    </div>

    <article>
      <h1 class="post-title">
        <a href="https://blog.octopusx.de/posts/kubes_and_gpus/">Kubes and GPUs</a>
      </h1>

      

      

      

      <div class="post-content">
        <p>Graphics cards are once again available. The AI hype is on for sure, but it&rsquo;s not as much of a craze as the the mining fad of yesteryear. Finally, graphics-based compute is semi-affordable again, especially if you look to the second hand market. Having dabbed my toes in the world of self hosting large language models I have come to realize that hosting those on CPUs only is not going to give me a good insight into what this tech is capable of. A lot of different models and APIs are either optimized or exclusively available on GPU platforms only. The one that tipped the scale for me is <a href="https://refact.ai">refact.ai</a>. I really wanted to give the self hosted version a spin, and so I had gone on to make my wallet slimmer.</p>
<h2 id="the-hardware">The Hardware</h2>
<p>What GPU you should buy to get the most out of it depends on the models that you want to run on it. There do exist smaller coding-focused LLMs that can be run on a GTX 1060 6GB or an 8GB GTX 1070, which can be sniped on the second hand market for under a 100 euros. AMD has historically sold cards with higher VRAM sizes which would make their second hand units the best bargain, sadly it seems that their <a href="https://rocmdocs.amd.com/en/latest/">ROCm software platform</a> for running AI compute tasks is still in its early days and not many, if any, consumer cards are supported at the moment.</p>
<p>Some of the available models and their estimated VRAM requirements:</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>VRAM Requirement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Refact/1.6B</td>
<td>~4 GB</td>
</tr>
<tr>
<td>starcoder/15b/base</td>
<td>~9 GB</td>
</tr>
<tr>
<td>wizardcoder/15b</td>
<td>~9 GB</td>
</tr>
<tr>
<td>codellama/7b</td>
<td>~14.3 GB</td>
</tr>
</tbody>
</table>
<p>I was actually about to pick up a second hand GTX 1070 with 8GB of VRAM for 90 euros when I found a good deal on a refurbished RTX 3060 12GB model and ordered immediately for 270 euros delivered. In terms of performance per euro the 1070 was a much better choice, however the 3060 has 3 things going for it:</p>
<ul>
<li>lower power consumption (lol nope)</li>
<li>smaller physical footprint (I am trying to cram this into a mITX case)</li>
<li>extra 4GB of VRAM (more VRAM more better models)</li>
</ul>
<p>Fun fact, now that I already have the card and started writing this article, I realized that actually the 3060 has a higher peak power and similar idle numbers to the 1070&hellip; Oh well&hellip; According to techPowerUp anyway: 1070: 13W idle, 148 peak, 3060: 13W idle, 179 peak (<a href="https://www.techpowerup.com/review/evga-geforce-rtx-3060-xc/36.html)">https://www.techpowerup.com/review/evga-geforce-rtx-3060-xc/36.html)</a>.</p>
<p>A lot of models that I want to try out exceed the 8GB mark once loaded into RAM, notably <code>starcoder</code>. I was also curious to figure out what it would take to set up CUDA time sharing on a single GPU, where multiple smaller models are loaded into VRAM simultaneously. It is a shame that AMD is not an option, as they have a number of 16GB VRAM cards at a similar price to this one (RX6700XT for example), while for Nvidia we would have to jump up to an RTX 4060 16GB, which retails at around 500 euros.</p>
<h2 id="the-platform">The Platform</h2>
<p>You know what you&rsquo;re getting yourself into coming here right? It&rsquo;s all about Kubernetes xD One of the Proxmox nodes in my server rack has a spare X16 slot that will happily house our refurb 3060, and we can pass it through to one of the Kubernetes node VMs. To do this we need to follow a few steps:</p>
<h3 id="prepare-proxmox">Prepare Proxmox</h3>
<ol>
<li>Drain the node to be shut down</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl drain k8s12 --delete-emptydir-data --ignore-daemonsets
</span></span></code></pre></div><ol start="2">
<li>
<p>Edit the hardware definition of the node (add the new PCI device)
<img src="proxmox.png" alt="adding pcie device to vm in proxmox"></p>
</li>
<li>
<p>Shut down the node so that it may pick up the hardware change</p>
</li>
<li>
<p>(optional) Make a snapshot/backup of the VM, in case we screw something up later
<img src="proxmox_snapshot.png" alt="creating a snapshot of the VM we are about to modify"></p>
</li>
<li>
<p>Boot it up again</p>
</li>
</ol>
<p>Voila, the GPU should be now available inside the VM. We can check that by running:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~ ❯ lspci                                                                             octopusx@k8s12
</span></span><span style="display:flex;"><span>00:00.0 Host bridge: Intel Corporation 440FX - 82441FX PMC <span style="color:#f92672">[</span>Natoma<span style="color:#f92672">]</span> <span style="color:#f92672">(</span>rev 02<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:01.0 ISA bridge: Intel Corporation 82371SB PIIX3 ISA <span style="color:#f92672">[</span>Natoma/Triton II<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>00:01.1 IDE interface: Intel Corporation 82371SB PIIX3 IDE <span style="color:#f92672">[</span>Natoma/Triton II<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>00:01.2 USB controller: Intel Corporation 82371SB PIIX3 USB <span style="color:#f92672">[</span>Natoma/Triton II<span style="color:#f92672">]</span> <span style="color:#f92672">(</span>rev 01<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:01.3 Bridge: Intel Corporation 82371AB/EB/MB PIIX4 ACPI <span style="color:#f92672">(</span>rev 03<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:02.0 VGA compatible controller: Device 1234:1111 <span style="color:#f92672">(</span>rev 02<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:03.0 Unclassified device <span style="color:#f92672">[</span>00ff<span style="color:#f92672">]</span>: Red Hat, Inc. Virtio memory balloon
</span></span><span style="display:flex;"><span>00:05.0 SCSI storage controller: Red Hat, Inc. Virtio SCSI
</span></span><span style="display:flex;"><span>00:10.0 VGA compatible controller: NVIDIA Corporation GA106 <span style="color:#f92672">[</span>GeForce RTX <span style="color:#ae81ff">3060</span> Lite Hash Rate<span style="color:#f92672">]</span> <span style="color:#f92672">(</span>rev a1<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:11.0 Audio device: NVIDIA Corporation Device 228e <span style="color:#f92672">(</span>rev a1<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>00:12.0 Ethernet controller: Red Hat, Inc. Virtio network device
</span></span><span style="display:flex;"><span>00:1e.0 PCI bridge: Red Hat, Inc. QEMU PCI-PCI bridge
</span></span><span style="display:flex;"><span>00:1f.0 PCI bridge: Red Hat, Inc. QEMU PCI-PCI bridge
</span></span></code></pre></div><ol start="6">
<li>Uncordon the K8S node to re-enable scheduling on it</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl uncordon k8s12
</span></span></code></pre></div><h4 id="fun-fact">Fun fact</h4>
<p>As much as the above steps are all that I &ldquo;technically&rdquo; had to do should my proxmox server be correctly configured from the get go, I was getting weird behavior at the start. The graphics card was passed into the VM the first time it booted, but the driver (I installed next) would tell me there is no GPU available, even after rebooting the VM. I started messing around, thinking it is maybe some weird property of PCIe passthrough on Nvidia hardware, went on to blacklist the nvidia kernel module/drivers inside the Proxmox host to make sure it doesn&rsquo;t get initialized by the host, which in turn prevented the VM I would pass it through from booting altogether. In the end I jumped into the bios of the machine to make sure all of the PCIe-related settings are in order and&hellip; of course I left the bifurcation settings on the x16 slot in the x8/x8 mode from when I was using multiple NVME drives in that slot. Changing that back and reverting all of the trial and error driver shenanigans on the host got it to work, boot and be available inside the VM every time.</p>
<h3 id="install-nvidia-drivers">Install Nvidia Drivers</h3>
<p>Now off to install all of that dirty proprietary Nvidia stuff. We only just gave them our money, so we&rsquo;ve got nothing to loose by going all the way and also giving them our souls.</p>
<p>Nvidia has a pretty solid guide with versions for various popular server linux flavors:</p>
<ul>
<li><a href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html">https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html</a></li>
</ul>
<p>Which amounts to installing the CUDA toolkit and drivers.</p>
<ol>
<li>Install the CUDA toolkit</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
</span></span><span style="display:flex;"><span>sudo dpkg -i cuda-keyring_1.1-1_all.deb
</span></span><span style="display:flex;"><span>sudo apt-get update
</span></span><span style="display:flex;"><span>sudo apt-get -y install cuda-toolkit-12-3
</span></span></code></pre></div><ol start="2">
<li>Install Nvidia drivers</li>
</ol>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>sudo apt-get install -y nvidia-kernel-open-545
</span></span><span style="display:flex;"><span>sudo apt-get install -y cuda-drivers-545
</span></span></code></pre></div><ol start="3">
<li>Install the Nvidia container runtime</li>
</ol>
<p>The last step that we need to do to prepare our Ubuntu VM to run GPU accelerated tasks inside OCI containers is to install the Nvidia container runtime. This is a container runtime that is compatible with the OCI specification and is able to run GPU accelerated containers. It is also compatible with the Kubernetes CRI interface, which is what K3S uses to run containers. Once installed, K3S will automatically detect it once its service is restarted.
You can find out the setup details (which are very simple) on the following documentation page: <a href="https://docs.k3s.io/advanced#configuring-containerd">https://docs.k3s.io/advanced#configuring-containerd</a>.</p>
<p>Since we are running K3S on Ubuntu we are using systemd-containerd and installing everything from an apt repo. Therefore we need to follow Nvidia&rsquo;s instructions for enabling their container runtime for containerd, as found here: <a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-containerd">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-containerd</a>. In my case that would be:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#f92672">&amp;&amp;</span> curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    sed <span style="color:#e6db74">&#39;s#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g&#39;</span> | <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>  <span style="color:#f92672">&amp;&amp;</span> <span style="color:#ae81ff">\
</span></span></span><span style="display:flex;"><span><span style="color:#ae81ff"></span>    sudo apt update
</span></span><span style="display:flex;"><span>    sudo apt install -y nvidia-container-toolkit
</span></span></code></pre></div><p>Once this is done you can run an Nvidia CUDA container to see that the runtime is functioning:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~ ❯ sudo ctr image pull docker.io/nvidia/cuda:12.2.2-base-ubuntu22.04
</span></span><span style="display:flex;"><span>docker.io/nvidia/cuda:12.2.2-base-ubuntu22.04:                                    resolved       |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>index-sha256:0b165c469e8b0a620ce6b22373ead52502ef06d3088ba35a6edb78582b5274f6:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>manifest-sha256:1f0416abdf40fca3a4ce4e42093584664b4ac0dddd012571453c94e5c7a35937: <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>config-sha256:f5861cf44882f74454e1a5915647321b5b344af925503515bcd8bb0728d84551:   <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:aece8493d3972efa43bfd4ee3cdba659c0f787f8f59c82fb3e48c87cbb22a12e:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:4b46bd5b7766c7415446d5b866f709b45dbdb60b04fec7677343d8232ca2e427:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:f038740a8e0591897bdd6280d24e1f89690cf80c28e77c14072f84eca205c7e6:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:b2dca9da14b18f8a959e7deed652de532f88f979d8d1279511b987e294699f3c:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>layer-sha256:998220aec9f5708b331daeb126f9d66fe39bbcc333224ca9ba6efabc9b563b7d:    <span style="color:#66d9ef">done</span>           |++++++++++++++++++++++++++++++++++++++| 
</span></span><span style="display:flex;"><span>elapsed: 7.0 s                                                                    total:  83.6 M <span style="color:#f92672">(</span>11.9 MiB/s<span style="color:#f92672">)</span>                                      
</span></span><span style="display:flex;"><span>unpacking linux/amd64 sha256:0b165c469e8b0a620ce6b22373ead52502ef06d3088ba35a6edb78582b5274f6...
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">done</span>: 1.00494491s
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>~ ❯ sudo ctr run --rm --gpus <span style="color:#ae81ff">0</span> -t docker.io/nvidia/cuda:12.2.2-base-ubuntu22.04 12.2.2-base-ubuntu22.04 nvidia-smi
</span></span><span style="display:flex;"><span>Thu Oct <span style="color:#ae81ff">26</span> 07:30:54 <span style="color:#ae81ff">2023</span>       
</span></span><span style="display:flex;"><span>+---------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| NVIDIA-SMI 545.23.06              Driver Version: 545.23.06    CUDA Version: 12.3     |
</span></span><span style="display:flex;"><span>|-----------------------------------------+----------------------+----------------------+
</span></span><span style="display:flex;"><span>| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
</span></span><span style="display:flex;"><span>| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
</span></span><span style="display:flex;"><span>|                                         |                      |               MIG M. |
</span></span><span style="display:flex;"><span>|<span style="color:#f92672">=========================================</span>+<span style="color:#f92672">======================</span>+<span style="color:#f92672">======================</span>|
</span></span><span style="display:flex;"><span>|   <span style="color:#ae81ff">0</span>  NVIDIA GeForce RTX <span style="color:#ae81ff">3060</span>        On  | 00000000:00:10.0 Off |                  N/A |
</span></span><span style="display:flex;"><span>|  0%   48C    P2              46W / 170W |  11651MiB / 12288MiB |      0%      Default |
</span></span><span style="display:flex;"><span>|                                         |                      |                  N/A |
</span></span><span style="display:flex;"><span>+-----------------------------------------+----------------------+----------------------+
</span></span><span style="display:flex;"><span>                                                                                         
</span></span><span style="display:flex;"><span>+---------------------------------------------------------------------------------------+
</span></span><span style="display:flex;"><span>| Processes:                                                                            |
</span></span><span style="display:flex;"><span>|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
</span></span><span style="display:flex;"><span>|        ID   ID                                                             Usage      |
</span></span><span style="display:flex;"><span>|<span style="color:#f92672">=======================================================================================</span>|
</span></span><span style="display:flex;"><span>|  No running processes found                                                           |
</span></span><span style="display:flex;"><span>+---------------------------------------------------------------------------------------+
</span></span></code></pre></div><p>And also, once you&rsquo;ve restarted your K3S agent service, check that it also picked up the change:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~ ❯ sudo grep nvidia /var/lib/rancher/k3s/agent/etc/containerd/config.toml              octopusx@k8s12
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.<span style="color:#e6db74">&#34;nvidia&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>plugins.<span style="color:#e6db74">&#34;io.containerd.grpc.v1.cri&#34;</span>.containerd.runtimes.<span style="color:#e6db74">&#34;nvidia&#34;</span>.options<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>  BinaryName <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/usr/bin/nvidia-container-runtime&#34;</span>
</span></span></code></pre></div><h1 id="the-deployment">The Deployment</h1>
<p>In order for any container to take advantage of the nvidia containerd runtime plugin we need to create a special runtime class object, which can be referenced inside our kubernetes deployment yaml.</p>
<p>It looks like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">node.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RuntimeClass</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nvidia</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">handler</span>: <span style="color:#ae81ff">nvidia</span>
</span></span></code></pre></div><p>Since (at least in my case) I am running a single GPU that is attached to a specific K8S worker VM, we need to somehow force the workloads that require access to said GPU to be deployed on that specific node. To do this we create a node label that we can then reference in the pod affinity specification:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>kubectl label nodes k8s12 gpu<span style="color:#f92672">=</span><span style="color:#ae81ff">3060</span>
</span></span></code></pre></div><p>We can check that it worked by invoking the describe command on the K8S node object:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>~/ ❯ kubectl describe node k8s12                                                                                                                       ○ teleport-k3s
</span></span><span style="display:flex;"><span>Name:               k8s12
</span></span><span style="display:flex;"><span>Roles:              worker
</span></span><span style="display:flex;"><span>Labels:             beta.kubernetes.io/arch<span style="color:#f92672">=</span>amd64
</span></span><span style="display:flex;"><span>                    beta.kubernetes.io/instance-type<span style="color:#f92672">=</span>k3s
</span></span><span style="display:flex;"><span>                    beta.kubernetes.io/os<span style="color:#f92672">=</span>linux
</span></span><span style="display:flex;"><span>                    gpu<span style="color:#f92672">=</span><span style="color:#ae81ff">3060</span>
</span></span><span style="display:flex;"><span>                    kubernetes.io/arch<span style="color:#f92672">=</span>amd64
</span></span><span style="display:flex;"><span>                    kubernetes.io/hostname<span style="color:#f92672">=</span>k8s12
</span></span><span style="display:flex;"><span>                    kubernetes.io/os<span style="color:#f92672">=</span>linux
</span></span><span style="display:flex;"><span>                    kubernetes.io/role<span style="color:#f92672">=</span>worker
</span></span><span style="display:flex;"><span>                    node.kubernetes.io/instance-type<span style="color:#f92672">=</span>k3s
</span></span></code></pre></div><p>You will see that I created a label with a non-binary value. This is to make this system a little more future-proof on my side. This way I can match either on the label just &ldquo;existing&rdquo;, so if I want any GPU present to bind a deployment onto, or to a specific value if I want to assign a workload to a predefined model. Neat.</p>
<p>To make sure that our assignment mechanism works, we can spawn a test GPU benchmark container. If we did everything right this should return us a benchmark score for our GPU:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nbody-gpu-benchmark</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">restartPolicy</span>: <span style="color:#ae81ff">OnFailure</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">runtimeClassName</span>: <span style="color:#ae81ff">nvidia</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cuda-container</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nvcr.io/nvidia/k8s/cuda-sample:nbody</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;nbody&#34;</span>, <span style="color:#e6db74">&#34;-gpu&#34;</span>, <span style="color:#e6db74">&#34;-benchmark&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">NVIDIA_VISIBLE_DEVICES</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">value</span>: <span style="color:#ae81ff">all</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">NVIDIA_DRIVER_CAPABILITIES</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">value</span>: <span style="color:#ae81ff">all</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">affinity</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">nodeAffinity</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">nodeSelectorTerms</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">matchExpressions</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">gpu</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">Exists</span>
</span></span></code></pre></div><h3 id="refactai-chart">RefactAI Chart</h3>
<p>Now that we have checked that everything necessary to deploy refact is in place, we deploy it. Here is an example output of <code>helm template .</code> command for the helm chart that I created:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e"># Source: refact/templates/service.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Service</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">helm.sh/chart</span>: <span style="color:#ae81ff">refact-0.1.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/managed-by</span>: <span style="color:#ae81ff">Helm</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">type</span>: <span style="color:#ae81ff">ClusterIP</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">port</span>: <span style="color:#ae81ff">8008</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">targetPort</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Source: refact/templates/deployment.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">helm.sh/chart</span>: <span style="color:#ae81ff">refact-0.1.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/managed-by</span>: <span style="color:#ae81ff">Helm</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">serviceName</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">revisionHistoryLimit</span>: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">hostNetwork</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">runtimeClassName</span>: <span style="color:#ae81ff">nvidia</span> <span style="color:#75715e"># IMPORTANT!</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">dnsPolicy</span>: <span style="color:#ae81ff">ClusterFirstWithHostNet</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">enableServiceLinks</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">image</span>: <span style="color:#e6db74">&#34;smallcloud/refact_self_hosting:latest&#34;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">imagePullPolicy</span>: <span style="color:#ae81ff">IfNotPresent</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">securityContext</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">privileged</span>: <span style="color:#66d9ef">false</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;TZ&#34;</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;UTC+02:00&#34;</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">http</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">8008</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">protocol</span>: <span style="color:#ae81ff">TCP</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">data</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/perm_storage</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">affinity</span>: <span style="color:#75715e"># This is how we tell it to only spawn on the node with our GPU</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">nodeAffinity</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">nodeSelectorTerms</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">matchExpressions</span>:
</span></span><span style="display:flex;"><span>              - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">gpu</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">Exists</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">data</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">persistentVolumeClaim</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Source: refact/templates/ingress.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">networking.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Ingress</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">helm.sh/chart</span>: <span style="color:#ae81ff">refact-0.1.0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/instance</span>: <span style="color:#ae81ff">release-name</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/version</span>: <span style="color:#e6db74">&#34;latest&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app.kubernetes.io/managed-by</span>: <span style="color:#ae81ff">Helm</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kubernetes.io/ingress.class</span>: <span style="color:#75715e"># insert your own ingress class to use</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># insert any other annotations you need for your ingress controller</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">tls</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">hosts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#ae81ff">refact.example.com</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">secretName</span>: <span style="color:#e6db74">&#34;wildcard-cert&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">host</span>: <span style="color:#ae81ff">refact.example.com</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">http</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">paths</span>:
</span></span><span style="display:flex;"><span>          - <span style="color:#f92672">path</span>: <span style="color:#e6db74">&#34;/&#34;</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">pathType</span>: <span style="color:#ae81ff">Prefix</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">backend</span>:
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">service</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">name</span>: <span style="color:#ae81ff">refact</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">port</span>:
</span></span><span style="display:flex;"><span>                  <span style="color:#f92672">number</span>: <span style="color:#ae81ff">8008</span>
</span></span></code></pre></div><p>For me an extra step here would be to add <code>refact.example.com</code> to my local DNS to expose this ingress entry (I just add it to my PiHole server).
If all goes well, you should be able to go to <code>refact.example.com</code> (taking that you have DNS set up for this) you should see a screen much like this:</p>
<p><img src="refact_web.png" alt="refact web UI"></p>
<p>Here I have loaded 2 small models, one that provides code completion and one that provides chat functionality.
Also, I have no idea why I am getting the VRAM warning message. It has been there for me since the beginning no matter what model I select and since everything is working for me I am going to assume it&rsquo;s a UI bug&hellip;</p>
<h3 id="vscode-plugin">VSCode Plugin</h3>
<p>Let&rsquo;s finally go harness the power of the AI in a practical manner. First, we install the refact.ai plugin:</p>
<p><img src="refact_vscode_plugin.png" alt="refact vscode plugin"></p>
<p>Then we go to its config page to enter our local instance URL:</p>
<p><img src="plugin_config.png" alt="plugin config"></p>
<p>And off you go! Refact should now automatically start generating suggestions in your VSCode editor window as you type:</p>
<p><img src="autocompletion.png" alt="autocompletion"></p>
<p>In my case, it will also allow you to chat with the llama7b model:</p>
<p><img src="llama7b_chat.png" alt="llama7b chat"></p>
<h2 id="the-end">The End</h2>
<p>All in all, I am quite happy with this setup. It was very straight forward and pretty much just worked. I may even sign up for their paid version just to support their project, well done refact.ai! I haven&rsquo;t used this in practice much so I don&rsquo;t feel comfortable giving a verdict on how good the small models that I am able to fit into my 3060&rsquo;s memory are. However what I can say is that the models execute extremely fast on the RTX 3060 12GB gpu&hellip; The code generation is very fast and the llama7b models chat is pretty much instant. Much more responsive than my previous attempt at CPU hosted model and downright impressive, with such a relatively small investment in hardware. Something I really wish I was able to do is expose my llama7b model hosted by refact in other apps via an openAI API endpoint. Another would be to be able to host a single larger model that does both code completion as well as chatting. That would make me 120% happy&hellip; For the moment, I have browsed the refact&rsquo;s github issue tracker as well as their discord server (<a href="https://www.smallcloud.ai/discord">https://www.smallcloud.ai/discord</a>) which seems to be active and will see if I can find out more about the project&rsquo;s roadmap and if the few functionalities that are missing will be added over time. Hope you enjoyed this tutorial and will come back again when the next entry rolls in.</p>

      </div>
    </article>

    <hr />

    <div class="post-info">
      
      

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text">
          <path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path>
          <polyline points="14 2 14 8 20 8"></polyline>
          <line x1="16" y1="13" x2="8" y2="13"></line>
          <line x1="16" y1="17" x2="8" y2="17"></line>
          <polyline points="10 9 9 9 8 9"></polyline>
        </svg>
        2451 Words
      </p>

      <p>
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar">
          <rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect>
          <line x1="16" y1="2" x2="16" y2="6"></line>
          <line x1="8" y1="2" x2="8" y2="6"></line>
          <line x1="3" y1="10" x2="21" y2="10"></line>
        </svg>
        
          2023-10-30 11:00
        

         
          
        
      </p>
    </div>

    
    <div class="pagination">
        
        <div class="pagination__title">
            <span class="pagination__title-h">Read other posts</span>
            <hr />
        </div>
        

        <div class="pagination__buttons">
            
            <span class="button previous">
                <a href="https://blog.octopusx.de/posts/teapot/">
                    <span class="button__icon">←</span>
                    <span class="button__text">Teapot - host your own helm chart registry</span>
                </a>
            </span>
            

            
            <span class="button next">
                <a href="https://blog.octopusx.de/posts/reasonably_large_language_models/">
                    <span class="button__text">Reasonably Large Language Models</span>
                    <span class="button__icon">→</span>
                </a>
            </span>
            
        </div>
    </div>


    

    

  </main>

            </div>

            
                <footer class="footer">
    
    <div class="footer__inner">
        <div class="footer__content">
            
            <span><a href="https://blog.octopusx.de/"></a></span>
            <span><a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener">CC BY-NC 4.0</a></span>
            <span><a href="https://blog.octopusx.de/posts/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 20 20" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a></span>
            
        </div>
    </div>
    
    
    <div class="footer__inner">
        <div class="footer__content">
            <span>Powered by <a href="http://gohugo.io">Hugo</a></span><span>Made with &#10084; by <a href="https://github.com/octopusx">Accidentally Competent</a></span>
        </div>
    </div>
    
</footer>

            
        </div>

        



<script type="text/javascript" src="../../bundle.min.bd0f0ac9666624c2a336739d6ea5e4bbdbc1915e288c3970cec82782d5095baf4541e629f0ee23052820f9bb967778058ed0b6c5f62334390306f11722e6923e.js" integrity="sha512-vQ8KyWZmJMKjNnOdbqXku9vBkV4ojDlwzsgngtUJW69FQeYp8O4jBSgg&#43;buWd3gFjtC2xfYjNDkDBvEXIuaSPg=="></script>



    </body>
</html>
